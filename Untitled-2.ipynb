{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9853ed99-6581-47dd-b4ea-b3ed17484f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install  -U -q git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git datasets bitsandbytes peft qwen-vl-utils wandb accelerate\n",
    "# Tested with transformers==4.47.0.dev0, trl==0.12.0.dev0, datasets==3.0.2, bitsandbytes==0.44.1, peft==0.13.2, qwen-vl-utils==0.0.8, wandb==0.18.5, accelerate==1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52136eac-d8e0-44bb-a04e-7370ec829d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torch==2.4.1+cu121 torchvision==0.19.1+cu121 torchaudio==2.4.1+cu121 --extra-index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ff2f43f-a17c-47ca-9a7c-6d5cb323c338",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a human evaluator choosing between two AI-generated images—image 1 (left) and image 2 (right)—produced from a text prompt. Critically compare both images and choose the better image.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cd05908-2693-445e-86fa-f38867d530bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"\"\"Task Structure:\n",
    "1. In a <think> block, compare the two images on each of five relative criteria:\n",
    "Alignment: Which image matches the prompt better?\n",
    "Object & Scene Correctness: Which has more believable objects and background?\n",
    "Image Quality (Photorealism): Which is sharper and free of defects?\n",
    "Aesthetic Appeal: Which has better composition, color, mood?\n",
    "Creativity & Originality: Which is more unique or storytelling?\n",
    "\n",
    "For each criterion, include a one-sentence comparison and then a Score chosen from:\n",
    "Strongly prefer image 1\n",
    "Prefer image 1\n",
    "Both images are preferred\n",
    "Prefer image 2\n",
    "Strongly prefer image 2\n",
    "\n",
    "2. In a <preference> block, output your overall choice—either '1' or '2'.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf280ef6-ec31-4785-81f5-b16986a67973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def format_data(sample):\n",
    "\n",
    "    prompt = sample[\"caption\"]\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": Image.open(BytesIO(sample[\"jpg_0\"])),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": Image.open(BytesIO(sample[\"jpg_1\"])),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"{user_message}. The images are generated from the prompt: {prompt}\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": sample[\"caption\"]}],\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a57b5eda-0f6f-41f8-9b40-718f9ca9d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_id = \"kevinkingslin/GRPODataset2\"\n",
    "train_dataset = load_dataset(dataset_id, split=[\"train\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9db7c113-884d-4c39-a2c8-f89d2c3509da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['are_different', 'best_image_uid', 'caption', 'created_at', 'has_label', 'image_0_uid', 'image_0_url', 'image_1_uid', 'image_1_url', 'jpg_0', 'jpg_1', 'label_0', 'label_1', 'model_0', 'model_1', 'ranking_id', 'user_id', 'num_example_per_prompt', '__index_level_0__', 'image1_better', 'image2_better'],\n",
       "    num_rows: 2583\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5596841f-48e7-43e7-86b8-1dada72781f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = [format_data(sample) for sample in train_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfcc60e8-e653-455a-b3e9-0f3bb6c1818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
    "\n",
    "model_id = \"Qwen/Qwen2-VL-2B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9cee6d4-53be-485f-88fe-d2b68760d456",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f27219598a94ea48bf77a4044f1a042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f694197db746a8bdc66cbb03176c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/56.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497f301303f7402eb4569c7833f69831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf92dd1b0a7947d5b7c8bc1e0918ac74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3afad04f100140d18fcd7b2388aa4349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/429M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "528c1169da924548874c7d64dbdb4938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Qwen/Qwen2-VL-2B-Instruct were not used when initializing Qwen2VLForConditionalGeneration: ['model.embed_tokens.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.9.self_attn.v_proj.weight', 'model.norm.weight', 'visual.blocks.0.attn.proj.bias', 'visual.blocks.0.attn.proj.weight', 'visual.blocks.0.attn.qkv.bias', 'visual.blocks.0.attn.qkv.weight', 'visual.blocks.0.mlp.fc1.bias', 'visual.blocks.0.mlp.fc1.weight', 'visual.blocks.0.mlp.fc2.bias', 'visual.blocks.0.mlp.fc2.weight', 'visual.blocks.0.norm1.bias', 'visual.blocks.0.norm1.weight', 'visual.blocks.0.norm2.bias', 'visual.blocks.0.norm2.weight', 'visual.blocks.1.attn.proj.bias', 'visual.blocks.1.attn.proj.weight', 'visual.blocks.1.attn.qkv.bias', 'visual.blocks.1.attn.qkv.weight', 'visual.blocks.1.mlp.fc1.bias', 'visual.blocks.1.mlp.fc1.weight', 'visual.blocks.1.mlp.fc2.bias', 'visual.blocks.1.mlp.fc2.weight', 'visual.blocks.1.norm1.bias', 'visual.blocks.1.norm1.weight', 'visual.blocks.1.norm2.bias', 'visual.blocks.1.norm2.weight', 'visual.blocks.10.attn.proj.bias', 'visual.blocks.10.attn.proj.weight', 'visual.blocks.10.attn.qkv.bias', 'visual.blocks.10.attn.qkv.weight', 'visual.blocks.10.mlp.fc1.bias', 'visual.blocks.10.mlp.fc1.weight', 'visual.blocks.10.mlp.fc2.bias', 'visual.blocks.10.mlp.fc2.weight', 'visual.blocks.10.norm1.bias', 'visual.blocks.10.norm1.weight', 'visual.blocks.10.norm2.bias', 'visual.blocks.10.norm2.weight', 'visual.blocks.11.attn.proj.bias', 'visual.blocks.11.attn.proj.weight', 'visual.blocks.11.attn.qkv.bias', 'visual.blocks.11.attn.qkv.weight', 'visual.blocks.11.mlp.fc1.bias', 'visual.blocks.11.mlp.fc1.weight', 'visual.blocks.11.mlp.fc2.bias', 'visual.blocks.11.mlp.fc2.weight', 'visual.blocks.11.norm1.bias', 'visual.blocks.11.norm1.weight', 'visual.blocks.11.norm2.bias', 'visual.blocks.11.norm2.weight', 'visual.blocks.12.attn.proj.bias', 'visual.blocks.12.attn.proj.weight', 'visual.blocks.12.attn.qkv.bias', 'visual.blocks.12.attn.qkv.weight', 'visual.blocks.12.mlp.fc1.bias', 'visual.blocks.12.mlp.fc1.weight', 'visual.blocks.12.mlp.fc2.bias', 'visual.blocks.12.mlp.fc2.weight', 'visual.blocks.12.norm1.bias', 'visual.blocks.12.norm1.weight', 'visual.blocks.12.norm2.bias', 'visual.blocks.12.norm2.weight', 'visual.blocks.13.attn.proj.bias', 'visual.blocks.13.attn.proj.weight', 'visual.blocks.13.attn.qkv.bias', 'visual.blocks.13.attn.qkv.weight', 'visual.blocks.13.mlp.fc1.bias', 'visual.blocks.13.mlp.fc1.weight', 'visual.blocks.13.mlp.fc2.bias', 'visual.blocks.13.mlp.fc2.weight', 'visual.blocks.13.norm1.bias', 'visual.blocks.13.norm1.weight', 'visual.blocks.13.norm2.bias', 'visual.blocks.13.norm2.weight', 'visual.blocks.14.attn.proj.bias', 'visual.blocks.14.attn.proj.weight', 'visual.blocks.14.attn.qkv.bias', 'visual.blocks.14.attn.qkv.weight', 'visual.blocks.14.mlp.fc1.bias', 'visual.blocks.14.mlp.fc1.weight', 'visual.blocks.14.mlp.fc2.bias', 'visual.blocks.14.mlp.fc2.weight', 'visual.blocks.14.norm1.bias', 'visual.blocks.14.norm1.weight', 'visual.blocks.14.norm2.bias', 'visual.blocks.14.norm2.weight', 'visual.blocks.15.attn.proj.bias', 'visual.blocks.15.attn.proj.weight', 'visual.blocks.15.attn.qkv.bias', 'visual.blocks.15.attn.qkv.weight', 'visual.blocks.15.mlp.fc1.bias', 'visual.blocks.15.mlp.fc1.weight', 'visual.blocks.15.mlp.fc2.bias', 'visual.blocks.15.mlp.fc2.weight', 'visual.blocks.15.norm1.bias', 'visual.blocks.15.norm1.weight', 'visual.blocks.15.norm2.bias', 'visual.blocks.15.norm2.weight', 'visual.blocks.16.attn.proj.bias', 'visual.blocks.16.attn.proj.weight', 'visual.blocks.16.attn.qkv.bias', 'visual.blocks.16.attn.qkv.weight', 'visual.blocks.16.mlp.fc1.bias', 'visual.blocks.16.mlp.fc1.weight', 'visual.blocks.16.mlp.fc2.bias', 'visual.blocks.16.mlp.fc2.weight', 'visual.blocks.16.norm1.bias', 'visual.blocks.16.norm1.weight', 'visual.blocks.16.norm2.bias', 'visual.blocks.16.norm2.weight', 'visual.blocks.17.attn.proj.bias', 'visual.blocks.17.attn.proj.weight', 'visual.blocks.17.attn.qkv.bias', 'visual.blocks.17.attn.qkv.weight', 'visual.blocks.17.mlp.fc1.bias', 'visual.blocks.17.mlp.fc1.weight', 'visual.blocks.17.mlp.fc2.bias', 'visual.blocks.17.mlp.fc2.weight', 'visual.blocks.17.norm1.bias', 'visual.blocks.17.norm1.weight', 'visual.blocks.17.norm2.bias', 'visual.blocks.17.norm2.weight', 'visual.blocks.18.attn.proj.bias', 'visual.blocks.18.attn.proj.weight', 'visual.blocks.18.attn.qkv.bias', 'visual.blocks.18.attn.qkv.weight', 'visual.blocks.18.mlp.fc1.bias', 'visual.blocks.18.mlp.fc1.weight', 'visual.blocks.18.mlp.fc2.bias', 'visual.blocks.18.mlp.fc2.weight', 'visual.blocks.18.norm1.bias', 'visual.blocks.18.norm1.weight', 'visual.blocks.18.norm2.bias', 'visual.blocks.18.norm2.weight', 'visual.blocks.19.attn.proj.bias', 'visual.blocks.19.attn.proj.weight', 'visual.blocks.19.attn.qkv.bias', 'visual.blocks.19.attn.qkv.weight', 'visual.blocks.19.mlp.fc1.bias', 'visual.blocks.19.mlp.fc1.weight', 'visual.blocks.19.mlp.fc2.bias', 'visual.blocks.19.mlp.fc2.weight', 'visual.blocks.19.norm1.bias', 'visual.blocks.19.norm1.weight', 'visual.blocks.19.norm2.bias', 'visual.blocks.19.norm2.weight', 'visual.blocks.2.attn.proj.bias', 'visual.blocks.2.attn.proj.weight', 'visual.blocks.2.attn.qkv.bias', 'visual.blocks.2.attn.qkv.weight', 'visual.blocks.2.mlp.fc1.bias', 'visual.blocks.2.mlp.fc1.weight', 'visual.blocks.2.mlp.fc2.bias', 'visual.blocks.2.mlp.fc2.weight', 'visual.blocks.2.norm1.bias', 'visual.blocks.2.norm1.weight', 'visual.blocks.2.norm2.bias', 'visual.blocks.2.norm2.weight', 'visual.blocks.20.attn.proj.bias', 'visual.blocks.20.attn.proj.weight', 'visual.blocks.20.attn.qkv.bias', 'visual.blocks.20.attn.qkv.weight', 'visual.blocks.20.mlp.fc1.bias', 'visual.blocks.20.mlp.fc1.weight', 'visual.blocks.20.mlp.fc2.bias', 'visual.blocks.20.mlp.fc2.weight', 'visual.blocks.20.norm1.bias', 'visual.blocks.20.norm1.weight', 'visual.blocks.20.norm2.bias', 'visual.blocks.20.norm2.weight', 'visual.blocks.21.attn.proj.bias', 'visual.blocks.21.attn.proj.weight', 'visual.blocks.21.attn.qkv.bias', 'visual.blocks.21.attn.qkv.weight', 'visual.blocks.21.mlp.fc1.bias', 'visual.blocks.21.mlp.fc1.weight', 'visual.blocks.21.mlp.fc2.bias', 'visual.blocks.21.mlp.fc2.weight', 'visual.blocks.21.norm1.bias', 'visual.blocks.21.norm1.weight', 'visual.blocks.21.norm2.bias', 'visual.blocks.21.norm2.weight', 'visual.blocks.22.attn.proj.bias', 'visual.blocks.22.attn.proj.weight', 'visual.blocks.22.attn.qkv.bias', 'visual.blocks.22.attn.qkv.weight', 'visual.blocks.22.mlp.fc1.bias', 'visual.blocks.22.mlp.fc1.weight', 'visual.blocks.22.mlp.fc2.bias', 'visual.blocks.22.mlp.fc2.weight', 'visual.blocks.22.norm1.bias', 'visual.blocks.22.norm1.weight', 'visual.blocks.22.norm2.bias', 'visual.blocks.22.norm2.weight', 'visual.blocks.23.attn.proj.bias', 'visual.blocks.23.attn.proj.weight', 'visual.blocks.23.attn.qkv.bias', 'visual.blocks.23.attn.qkv.weight', 'visual.blocks.23.mlp.fc1.bias', 'visual.blocks.23.mlp.fc1.weight', 'visual.blocks.23.mlp.fc2.bias', 'visual.blocks.23.mlp.fc2.weight', 'visual.blocks.23.norm1.bias', 'visual.blocks.23.norm1.weight', 'visual.blocks.23.norm2.bias', 'visual.blocks.23.norm2.weight', 'visual.blocks.24.attn.proj.bias', 'visual.blocks.24.attn.proj.weight', 'visual.blocks.24.attn.qkv.bias', 'visual.blocks.24.attn.qkv.weight', 'visual.blocks.24.mlp.fc1.bias', 'visual.blocks.24.mlp.fc1.weight', 'visual.blocks.24.mlp.fc2.bias', 'visual.blocks.24.mlp.fc2.weight', 'visual.blocks.24.norm1.bias', 'visual.blocks.24.norm1.weight', 'visual.blocks.24.norm2.bias', 'visual.blocks.24.norm2.weight', 'visual.blocks.25.attn.proj.bias', 'visual.blocks.25.attn.proj.weight', 'visual.blocks.25.attn.qkv.bias', 'visual.blocks.25.attn.qkv.weight', 'visual.blocks.25.mlp.fc1.bias', 'visual.blocks.25.mlp.fc1.weight', 'visual.blocks.25.mlp.fc2.bias', 'visual.blocks.25.mlp.fc2.weight', 'visual.blocks.25.norm1.bias', 'visual.blocks.25.norm1.weight', 'visual.blocks.25.norm2.bias', 'visual.blocks.25.norm2.weight', 'visual.blocks.26.attn.proj.bias', 'visual.blocks.26.attn.proj.weight', 'visual.blocks.26.attn.qkv.bias', 'visual.blocks.26.attn.qkv.weight', 'visual.blocks.26.mlp.fc1.bias', 'visual.blocks.26.mlp.fc1.weight', 'visual.blocks.26.mlp.fc2.bias', 'visual.blocks.26.mlp.fc2.weight', 'visual.blocks.26.norm1.bias', 'visual.blocks.26.norm1.weight', 'visual.blocks.26.norm2.bias', 'visual.blocks.26.norm2.weight', 'visual.blocks.27.attn.proj.bias', 'visual.blocks.27.attn.proj.weight', 'visual.blocks.27.attn.qkv.bias', 'visual.blocks.27.attn.qkv.weight', 'visual.blocks.27.mlp.fc1.bias', 'visual.blocks.27.mlp.fc1.weight', 'visual.blocks.27.mlp.fc2.bias', 'visual.blocks.27.mlp.fc2.weight', 'visual.blocks.27.norm1.bias', 'visual.blocks.27.norm1.weight', 'visual.blocks.27.norm2.bias', 'visual.blocks.27.norm2.weight', 'visual.blocks.28.attn.proj.bias', 'visual.blocks.28.attn.proj.weight', 'visual.blocks.28.attn.qkv.bias', 'visual.blocks.28.attn.qkv.weight', 'visual.blocks.28.mlp.fc1.bias', 'visual.blocks.28.mlp.fc1.weight', 'visual.blocks.28.mlp.fc2.bias', 'visual.blocks.28.mlp.fc2.weight', 'visual.blocks.28.norm1.bias', 'visual.blocks.28.norm1.weight', 'visual.blocks.28.norm2.bias', 'visual.blocks.28.norm2.weight', 'visual.blocks.29.attn.proj.bias', 'visual.blocks.29.attn.proj.weight', 'visual.blocks.29.attn.qkv.bias', 'visual.blocks.29.attn.qkv.weight', 'visual.blocks.29.mlp.fc1.bias', 'visual.blocks.29.mlp.fc1.weight', 'visual.blocks.29.mlp.fc2.bias', 'visual.blocks.29.mlp.fc2.weight', 'visual.blocks.29.norm1.bias', 'visual.blocks.29.norm1.weight', 'visual.blocks.29.norm2.bias', 'visual.blocks.29.norm2.weight', 'visual.blocks.3.attn.proj.bias', 'visual.blocks.3.attn.proj.weight', 'visual.blocks.3.attn.qkv.bias', 'visual.blocks.3.attn.qkv.weight', 'visual.blocks.3.mlp.fc1.bias', 'visual.blocks.3.mlp.fc1.weight', 'visual.blocks.3.mlp.fc2.bias', 'visual.blocks.3.mlp.fc2.weight', 'visual.blocks.3.norm1.bias', 'visual.blocks.3.norm1.weight', 'visual.blocks.3.norm2.bias', 'visual.blocks.3.norm2.weight', 'visual.blocks.30.attn.proj.bias', 'visual.blocks.30.attn.proj.weight', 'visual.blocks.30.attn.qkv.bias', 'visual.blocks.30.attn.qkv.weight', 'visual.blocks.30.mlp.fc1.bias', 'visual.blocks.30.mlp.fc1.weight', 'visual.blocks.30.mlp.fc2.bias', 'visual.blocks.30.mlp.fc2.weight', 'visual.blocks.30.norm1.bias', 'visual.blocks.30.norm1.weight', 'visual.blocks.30.norm2.bias', 'visual.blocks.30.norm2.weight', 'visual.blocks.31.attn.proj.bias', 'visual.blocks.31.attn.proj.weight', 'visual.blocks.31.attn.qkv.bias', 'visual.blocks.31.attn.qkv.weight', 'visual.blocks.31.mlp.fc1.bias', 'visual.blocks.31.mlp.fc1.weight', 'visual.blocks.31.mlp.fc2.bias', 'visual.blocks.31.mlp.fc2.weight', 'visual.blocks.31.norm1.bias', 'visual.blocks.31.norm1.weight', 'visual.blocks.31.norm2.bias', 'visual.blocks.31.norm2.weight', 'visual.blocks.4.attn.proj.bias', 'visual.blocks.4.attn.proj.weight', 'visual.blocks.4.attn.qkv.bias', 'visual.blocks.4.attn.qkv.weight', 'visual.blocks.4.mlp.fc1.bias', 'visual.blocks.4.mlp.fc1.weight', 'visual.blocks.4.mlp.fc2.bias', 'visual.blocks.4.mlp.fc2.weight', 'visual.blocks.4.norm1.bias', 'visual.blocks.4.norm1.weight', 'visual.blocks.4.norm2.bias', 'visual.blocks.4.norm2.weight', 'visual.blocks.5.attn.proj.bias', 'visual.blocks.5.attn.proj.weight', 'visual.blocks.5.attn.qkv.bias', 'visual.blocks.5.attn.qkv.weight', 'visual.blocks.5.mlp.fc1.bias', 'visual.blocks.5.mlp.fc1.weight', 'visual.blocks.5.mlp.fc2.bias', 'visual.blocks.5.mlp.fc2.weight', 'visual.blocks.5.norm1.bias', 'visual.blocks.5.norm1.weight', 'visual.blocks.5.norm2.bias', 'visual.blocks.5.norm2.weight', 'visual.blocks.6.attn.proj.bias', 'visual.blocks.6.attn.proj.weight', 'visual.blocks.6.attn.qkv.bias', 'visual.blocks.6.attn.qkv.weight', 'visual.blocks.6.mlp.fc1.bias', 'visual.blocks.6.mlp.fc1.weight', 'visual.blocks.6.mlp.fc2.bias', 'visual.blocks.6.mlp.fc2.weight', 'visual.blocks.6.norm1.bias', 'visual.blocks.6.norm1.weight', 'visual.blocks.6.norm2.bias', 'visual.blocks.6.norm2.weight', 'visual.blocks.7.attn.proj.bias', 'visual.blocks.7.attn.proj.weight', 'visual.blocks.7.attn.qkv.bias', 'visual.blocks.7.attn.qkv.weight', 'visual.blocks.7.mlp.fc1.bias', 'visual.blocks.7.mlp.fc1.weight', 'visual.blocks.7.mlp.fc2.bias', 'visual.blocks.7.mlp.fc2.weight', 'visual.blocks.7.norm1.bias', 'visual.blocks.7.norm1.weight', 'visual.blocks.7.norm2.bias', 'visual.blocks.7.norm2.weight', 'visual.blocks.8.attn.proj.bias', 'visual.blocks.8.attn.proj.weight', 'visual.blocks.8.attn.qkv.bias', 'visual.blocks.8.attn.qkv.weight', 'visual.blocks.8.mlp.fc1.bias', 'visual.blocks.8.mlp.fc1.weight', 'visual.blocks.8.mlp.fc2.bias', 'visual.blocks.8.mlp.fc2.weight', 'visual.blocks.8.norm1.bias', 'visual.blocks.8.norm1.weight', 'visual.blocks.8.norm2.bias', 'visual.blocks.8.norm2.weight', 'visual.blocks.9.attn.proj.bias', 'visual.blocks.9.attn.proj.weight', 'visual.blocks.9.attn.qkv.bias', 'visual.blocks.9.attn.qkv.weight', 'visual.blocks.9.mlp.fc1.bias', 'visual.blocks.9.mlp.fc1.weight', 'visual.blocks.9.mlp.fc2.bias', 'visual.blocks.9.mlp.fc2.weight', 'visual.blocks.9.norm1.bias', 'visual.blocks.9.norm1.weight', 'visual.blocks.9.norm2.bias', 'visual.blocks.9.norm2.weight', 'visual.merger.ln_q.bias', 'visual.merger.ln_q.weight', 'visual.merger.mlp.0.bias', 'visual.merger.mlp.0.weight', 'visual.merger.mlp.2.bias', 'visual.merger.mlp.2.weight', 'visual.patch_embed.proj.weight']\n",
      "- This IS expected if you are initializing Qwen2VLForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Qwen2VLForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Qwen2VLForConditionalGeneration were not initialized from the model checkpoint at Qwen/Qwen2-VL-2B-Instruct and are newly initialized: ['lm_head.weight', 'model.language_model.embed_tokens.weight', 'model.language_model.layers.0.input_layernorm.weight', 'model.language_model.layers.0.mlp.down_proj.weight', 'model.language_model.layers.0.mlp.gate_proj.weight', 'model.language_model.layers.0.mlp.up_proj.weight', 'model.language_model.layers.0.post_attention_layernorm.weight', 'model.language_model.layers.0.self_attn.k_proj.bias', 'model.language_model.layers.0.self_attn.k_proj.weight', 'model.language_model.layers.0.self_attn.o_proj.weight', 'model.language_model.layers.0.self_attn.q_proj.bias', 'model.language_model.layers.0.self_attn.q_proj.weight', 'model.language_model.layers.0.self_attn.v_proj.bias', 'model.language_model.layers.0.self_attn.v_proj.weight', 'model.language_model.layers.1.input_layernorm.weight', 'model.language_model.layers.1.mlp.down_proj.weight', 'model.language_model.layers.1.mlp.gate_proj.weight', 'model.language_model.layers.1.mlp.up_proj.weight', 'model.language_model.layers.1.post_attention_layernorm.weight', 'model.language_model.layers.1.self_attn.k_proj.bias', 'model.language_model.layers.1.self_attn.k_proj.weight', 'model.language_model.layers.1.self_attn.o_proj.weight', 'model.language_model.layers.1.self_attn.q_proj.bias', 'model.language_model.layers.1.self_attn.q_proj.weight', 'model.language_model.layers.1.self_attn.v_proj.bias', 'model.language_model.layers.1.self_attn.v_proj.weight', 'model.language_model.layers.10.input_layernorm.weight', 'model.language_model.layers.10.mlp.down_proj.weight', 'model.language_model.layers.10.mlp.gate_proj.weight', 'model.language_model.layers.10.mlp.up_proj.weight', 'model.language_model.layers.10.post_attention_layernorm.weight', 'model.language_model.layers.10.self_attn.k_proj.bias', 'model.language_model.layers.10.self_attn.k_proj.weight', 'model.language_model.layers.10.self_attn.o_proj.weight', 'model.language_model.layers.10.self_attn.q_proj.bias', 'model.language_model.layers.10.self_attn.q_proj.weight', 'model.language_model.layers.10.self_attn.v_proj.bias', 'model.language_model.layers.10.self_attn.v_proj.weight', 'model.language_model.layers.11.input_layernorm.weight', 'model.language_model.layers.11.mlp.down_proj.weight', 'model.language_model.layers.11.mlp.gate_proj.weight', 'model.language_model.layers.11.mlp.up_proj.weight', 'model.language_model.layers.11.post_attention_layernorm.weight', 'model.language_model.layers.11.self_attn.k_proj.bias', 'model.language_model.layers.11.self_attn.k_proj.weight', 'model.language_model.layers.11.self_attn.o_proj.weight', 'model.language_model.layers.11.self_attn.q_proj.bias', 'model.language_model.layers.11.self_attn.q_proj.weight', 'model.language_model.layers.11.self_attn.v_proj.bias', 'model.language_model.layers.11.self_attn.v_proj.weight', 'model.language_model.layers.12.input_layernorm.weight', 'model.language_model.layers.12.mlp.down_proj.weight', 'model.language_model.layers.12.mlp.gate_proj.weight', 'model.language_model.layers.12.mlp.up_proj.weight', 'model.language_model.layers.12.post_attention_layernorm.weight', 'model.language_model.layers.12.self_attn.k_proj.bias', 'model.language_model.layers.12.self_attn.k_proj.weight', 'model.language_model.layers.12.self_attn.o_proj.weight', 'model.language_model.layers.12.self_attn.q_proj.bias', 'model.language_model.layers.12.self_attn.q_proj.weight', 'model.language_model.layers.12.self_attn.v_proj.bias', 'model.language_model.layers.12.self_attn.v_proj.weight', 'model.language_model.layers.13.input_layernorm.weight', 'model.language_model.layers.13.mlp.down_proj.weight', 'model.language_model.layers.13.mlp.gate_proj.weight', 'model.language_model.layers.13.mlp.up_proj.weight', 'model.language_model.layers.13.post_attention_layernorm.weight', 'model.language_model.layers.13.self_attn.k_proj.bias', 'model.language_model.layers.13.self_attn.k_proj.weight', 'model.language_model.layers.13.self_attn.o_proj.weight', 'model.language_model.layers.13.self_attn.q_proj.bias', 'model.language_model.layers.13.self_attn.q_proj.weight', 'model.language_model.layers.13.self_attn.v_proj.bias', 'model.language_model.layers.13.self_attn.v_proj.weight', 'model.language_model.layers.14.input_layernorm.weight', 'model.language_model.layers.14.mlp.down_proj.weight', 'model.language_model.layers.14.mlp.gate_proj.weight', 'model.language_model.layers.14.mlp.up_proj.weight', 'model.language_model.layers.14.post_attention_layernorm.weight', 'model.language_model.layers.14.self_attn.k_proj.bias', 'model.language_model.layers.14.self_attn.k_proj.weight', 'model.language_model.layers.14.self_attn.o_proj.weight', 'model.language_model.layers.14.self_attn.q_proj.bias', 'model.language_model.layers.14.self_attn.q_proj.weight', 'model.language_model.layers.14.self_attn.v_proj.bias', 'model.language_model.layers.14.self_attn.v_proj.weight', 'model.language_model.layers.15.input_layernorm.weight', 'model.language_model.layers.15.mlp.down_proj.weight', 'model.language_model.layers.15.mlp.gate_proj.weight', 'model.language_model.layers.15.mlp.up_proj.weight', 'model.language_model.layers.15.post_attention_layernorm.weight', 'model.language_model.layers.15.self_attn.k_proj.bias', 'model.language_model.layers.15.self_attn.k_proj.weight', 'model.language_model.layers.15.self_attn.o_proj.weight', 'model.language_model.layers.15.self_attn.q_proj.bias', 'model.language_model.layers.15.self_attn.q_proj.weight', 'model.language_model.layers.15.self_attn.v_proj.bias', 'model.language_model.layers.15.self_attn.v_proj.weight', 'model.language_model.layers.16.input_layernorm.weight', 'model.language_model.layers.16.mlp.down_proj.weight', 'model.language_model.layers.16.mlp.gate_proj.weight', 'model.language_model.layers.16.mlp.up_proj.weight', 'model.language_model.layers.16.post_attention_layernorm.weight', 'model.language_model.layers.16.self_attn.k_proj.bias', 'model.language_model.layers.16.self_attn.k_proj.weight', 'model.language_model.layers.16.self_attn.o_proj.weight', 'model.language_model.layers.16.self_attn.q_proj.bias', 'model.language_model.layers.16.self_attn.q_proj.weight', 'model.language_model.layers.16.self_attn.v_proj.bias', 'model.language_model.layers.16.self_attn.v_proj.weight', 'model.language_model.layers.17.input_layernorm.weight', 'model.language_model.layers.17.mlp.down_proj.weight', 'model.language_model.layers.17.mlp.gate_proj.weight', 'model.language_model.layers.17.mlp.up_proj.weight', 'model.language_model.layers.17.post_attention_layernorm.weight', 'model.language_model.layers.17.self_attn.k_proj.bias', 'model.language_model.layers.17.self_attn.k_proj.weight', 'model.language_model.layers.17.self_attn.o_proj.weight', 'model.language_model.layers.17.self_attn.q_proj.bias', 'model.language_model.layers.17.self_attn.q_proj.weight', 'model.language_model.layers.17.self_attn.v_proj.bias', 'model.language_model.layers.17.self_attn.v_proj.weight', 'model.language_model.layers.18.input_layernorm.weight', 'model.language_model.layers.18.mlp.down_proj.weight', 'model.language_model.layers.18.mlp.gate_proj.weight', 'model.language_model.layers.18.mlp.up_proj.weight', 'model.language_model.layers.18.post_attention_layernorm.weight', 'model.language_model.layers.18.self_attn.k_proj.bias', 'model.language_model.layers.18.self_attn.k_proj.weight', 'model.language_model.layers.18.self_attn.o_proj.weight', 'model.language_model.layers.18.self_attn.q_proj.bias', 'model.language_model.layers.18.self_attn.q_proj.weight', 'model.language_model.layers.18.self_attn.v_proj.bias', 'model.language_model.layers.18.self_attn.v_proj.weight', 'model.language_model.layers.19.input_layernorm.weight', 'model.language_model.layers.19.mlp.down_proj.weight', 'model.language_model.layers.19.mlp.gate_proj.weight', 'model.language_model.layers.19.mlp.up_proj.weight', 'model.language_model.layers.19.post_attention_layernorm.weight', 'model.language_model.layers.19.self_attn.k_proj.bias', 'model.language_model.layers.19.self_attn.k_proj.weight', 'model.language_model.layers.19.self_attn.o_proj.weight', 'model.language_model.layers.19.self_attn.q_proj.bias', 'model.language_model.layers.19.self_attn.q_proj.weight', 'model.language_model.layers.19.self_attn.v_proj.bias', 'model.language_model.layers.19.self_attn.v_proj.weight', 'model.language_model.layers.2.input_layernorm.weight', 'model.language_model.layers.2.mlp.down_proj.weight', 'model.language_model.layers.2.mlp.gate_proj.weight', 'model.language_model.layers.2.mlp.up_proj.weight', 'model.language_model.layers.2.post_attention_layernorm.weight', 'model.language_model.layers.2.self_attn.k_proj.bias', 'model.language_model.layers.2.self_attn.k_proj.weight', 'model.language_model.layers.2.self_attn.o_proj.weight', 'model.language_model.layers.2.self_attn.q_proj.bias', 'model.language_model.layers.2.self_attn.q_proj.weight', 'model.language_model.layers.2.self_attn.v_proj.bias', 'model.language_model.layers.2.self_attn.v_proj.weight', 'model.language_model.layers.20.input_layernorm.weight', 'model.language_model.layers.20.mlp.down_proj.weight', 'model.language_model.layers.20.mlp.gate_proj.weight', 'model.language_model.layers.20.mlp.up_proj.weight', 'model.language_model.layers.20.post_attention_layernorm.weight', 'model.language_model.layers.20.self_attn.k_proj.bias', 'model.language_model.layers.20.self_attn.k_proj.weight', 'model.language_model.layers.20.self_attn.o_proj.weight', 'model.language_model.layers.20.self_attn.q_proj.bias', 'model.language_model.layers.20.self_attn.q_proj.weight', 'model.language_model.layers.20.self_attn.v_proj.bias', 'model.language_model.layers.20.self_attn.v_proj.weight', 'model.language_model.layers.21.input_layernorm.weight', 'model.language_model.layers.21.mlp.down_proj.weight', 'model.language_model.layers.21.mlp.gate_proj.weight', 'model.language_model.layers.21.mlp.up_proj.weight', 'model.language_model.layers.21.post_attention_layernorm.weight', 'model.language_model.layers.21.self_attn.k_proj.bias', 'model.language_model.layers.21.self_attn.k_proj.weight', 'model.language_model.layers.21.self_attn.o_proj.weight', 'model.language_model.layers.21.self_attn.q_proj.bias', 'model.language_model.layers.21.self_attn.q_proj.weight', 'model.language_model.layers.21.self_attn.v_proj.bias', 'model.language_model.layers.21.self_attn.v_proj.weight', 'model.language_model.layers.22.input_layernorm.weight', 'model.language_model.layers.22.mlp.down_proj.weight', 'model.language_model.layers.22.mlp.gate_proj.weight', 'model.language_model.layers.22.mlp.up_proj.weight', 'model.language_model.layers.22.post_attention_layernorm.weight', 'model.language_model.layers.22.self_attn.k_proj.bias', 'model.language_model.layers.22.self_attn.k_proj.weight', 'model.language_model.layers.22.self_attn.o_proj.weight', 'model.language_model.layers.22.self_attn.q_proj.bias', 'model.language_model.layers.22.self_attn.q_proj.weight', 'model.language_model.layers.22.self_attn.v_proj.bias', 'model.language_model.layers.22.self_attn.v_proj.weight', 'model.language_model.layers.23.input_layernorm.weight', 'model.language_model.layers.23.mlp.down_proj.weight', 'model.language_model.layers.23.mlp.gate_proj.weight', 'model.language_model.layers.23.mlp.up_proj.weight', 'model.language_model.layers.23.post_attention_layernorm.weight', 'model.language_model.layers.23.self_attn.k_proj.bias', 'model.language_model.layers.23.self_attn.k_proj.weight', 'model.language_model.layers.23.self_attn.o_proj.weight', 'model.language_model.layers.23.self_attn.q_proj.bias', 'model.language_model.layers.23.self_attn.q_proj.weight', 'model.language_model.layers.23.self_attn.v_proj.bias', 'model.language_model.layers.23.self_attn.v_proj.weight', 'model.language_model.layers.24.input_layernorm.weight', 'model.language_model.layers.24.mlp.down_proj.weight', 'model.language_model.layers.24.mlp.gate_proj.weight', 'model.language_model.layers.24.mlp.up_proj.weight', 'model.language_model.layers.24.post_attention_layernorm.weight', 'model.language_model.layers.24.self_attn.k_proj.bias', 'model.language_model.layers.24.self_attn.k_proj.weight', 'model.language_model.layers.24.self_attn.o_proj.weight', 'model.language_model.layers.24.self_attn.q_proj.bias', 'model.language_model.layers.24.self_attn.q_proj.weight', 'model.language_model.layers.24.self_attn.v_proj.bias', 'model.language_model.layers.24.self_attn.v_proj.weight', 'model.language_model.layers.25.input_layernorm.weight', 'model.language_model.layers.25.mlp.down_proj.weight', 'model.language_model.layers.25.mlp.gate_proj.weight', 'model.language_model.layers.25.mlp.up_proj.weight', 'model.language_model.layers.25.post_attention_layernorm.weight', 'model.language_model.layers.25.self_attn.k_proj.bias', 'model.language_model.layers.25.self_attn.k_proj.weight', 'model.language_model.layers.25.self_attn.o_proj.weight', 'model.language_model.layers.25.self_attn.q_proj.bias', 'model.language_model.layers.25.self_attn.q_proj.weight', 'model.language_model.layers.25.self_attn.v_proj.bias', 'model.language_model.layers.25.self_attn.v_proj.weight', 'model.language_model.layers.26.input_layernorm.weight', 'model.language_model.layers.26.mlp.down_proj.weight', 'model.language_model.layers.26.mlp.gate_proj.weight', 'model.language_model.layers.26.mlp.up_proj.weight', 'model.language_model.layers.26.post_attention_layernorm.weight', 'model.language_model.layers.26.self_attn.k_proj.bias', 'model.language_model.layers.26.self_attn.k_proj.weight', 'model.language_model.layers.26.self_attn.o_proj.weight', 'model.language_model.layers.26.self_attn.q_proj.bias', 'model.language_model.layers.26.self_attn.q_proj.weight', 'model.language_model.layers.26.self_attn.v_proj.bias', 'model.language_model.layers.26.self_attn.v_proj.weight', 'model.language_model.layers.27.input_layernorm.weight', 'model.language_model.layers.27.mlp.down_proj.weight', 'model.language_model.layers.27.mlp.gate_proj.weight', 'model.language_model.layers.27.mlp.up_proj.weight', 'model.language_model.layers.27.post_attention_layernorm.weight', 'model.language_model.layers.27.self_attn.k_proj.bias', 'model.language_model.layers.27.self_attn.k_proj.weight', 'model.language_model.layers.27.self_attn.o_proj.weight', 'model.language_model.layers.27.self_attn.q_proj.bias', 'model.language_model.layers.27.self_attn.q_proj.weight', 'model.language_model.layers.27.self_attn.v_proj.bias', 'model.language_model.layers.27.self_attn.v_proj.weight', 'model.language_model.layers.3.input_layernorm.weight', 'model.language_model.layers.3.mlp.down_proj.weight', 'model.language_model.layers.3.mlp.gate_proj.weight', 'model.language_model.layers.3.mlp.up_proj.weight', 'model.language_model.layers.3.post_attention_layernorm.weight', 'model.language_model.layers.3.self_attn.k_proj.bias', 'model.language_model.layers.3.self_attn.k_proj.weight', 'model.language_model.layers.3.self_attn.o_proj.weight', 'model.language_model.layers.3.self_attn.q_proj.bias', 'model.language_model.layers.3.self_attn.q_proj.weight', 'model.language_model.layers.3.self_attn.v_proj.bias', 'model.language_model.layers.3.self_attn.v_proj.weight', 'model.language_model.layers.4.input_layernorm.weight', 'model.language_model.layers.4.mlp.down_proj.weight', 'model.language_model.layers.4.mlp.gate_proj.weight', 'model.language_model.layers.4.mlp.up_proj.weight', 'model.language_model.layers.4.post_attention_layernorm.weight', 'model.language_model.layers.4.self_attn.k_proj.bias', 'model.language_model.layers.4.self_attn.k_proj.weight', 'model.language_model.layers.4.self_attn.o_proj.weight', 'model.language_model.layers.4.self_attn.q_proj.bias', 'model.language_model.layers.4.self_attn.q_proj.weight', 'model.language_model.layers.4.self_attn.v_proj.bias', 'model.language_model.layers.4.self_attn.v_proj.weight', 'model.language_model.layers.5.input_layernorm.weight', 'model.language_model.layers.5.mlp.down_proj.weight', 'model.language_model.layers.5.mlp.gate_proj.weight', 'model.language_model.layers.5.mlp.up_proj.weight', 'model.language_model.layers.5.post_attention_layernorm.weight', 'model.language_model.layers.5.self_attn.k_proj.bias', 'model.language_model.layers.5.self_attn.k_proj.weight', 'model.language_model.layers.5.self_attn.o_proj.weight', 'model.language_model.layers.5.self_attn.q_proj.bias', 'model.language_model.layers.5.self_attn.q_proj.weight', 'model.language_model.layers.5.self_attn.v_proj.bias', 'model.language_model.layers.5.self_attn.v_proj.weight', 'model.language_model.layers.6.input_layernorm.weight', 'model.language_model.layers.6.mlp.down_proj.weight', 'model.language_model.layers.6.mlp.gate_proj.weight', 'model.language_model.layers.6.mlp.up_proj.weight', 'model.language_model.layers.6.post_attention_layernorm.weight', 'model.language_model.layers.6.self_attn.k_proj.bias', 'model.language_model.layers.6.self_attn.k_proj.weight', 'model.language_model.layers.6.self_attn.o_proj.weight', 'model.language_model.layers.6.self_attn.q_proj.bias', 'model.language_model.layers.6.self_attn.q_proj.weight', 'model.language_model.layers.6.self_attn.v_proj.bias', 'model.language_model.layers.6.self_attn.v_proj.weight', 'model.language_model.layers.7.input_layernorm.weight', 'model.language_model.layers.7.mlp.down_proj.weight', 'model.language_model.layers.7.mlp.gate_proj.weight', 'model.language_model.layers.7.mlp.up_proj.weight', 'model.language_model.layers.7.post_attention_layernorm.weight', 'model.language_model.layers.7.self_attn.k_proj.bias', 'model.language_model.layers.7.self_attn.k_proj.weight', 'model.language_model.layers.7.self_attn.o_proj.weight', 'model.language_model.layers.7.self_attn.q_proj.bias', 'model.language_model.layers.7.self_attn.q_proj.weight', 'model.language_model.layers.7.self_attn.v_proj.bias', 'model.language_model.layers.7.self_attn.v_proj.weight', 'model.language_model.layers.8.input_layernorm.weight', 'model.language_model.layers.8.mlp.down_proj.weight', 'model.language_model.layers.8.mlp.gate_proj.weight', 'model.language_model.layers.8.mlp.up_proj.weight', 'model.language_model.layers.8.post_attention_layernorm.weight', 'model.language_model.layers.8.self_attn.k_proj.bias', 'model.language_model.layers.8.self_attn.k_proj.weight', 'model.language_model.layers.8.self_attn.o_proj.weight', 'model.language_model.layers.8.self_attn.q_proj.bias', 'model.language_model.layers.8.self_attn.q_proj.weight', 'model.language_model.layers.8.self_attn.v_proj.bias', 'model.language_model.layers.8.self_attn.v_proj.weight', 'model.language_model.layers.9.input_layernorm.weight', 'model.language_model.layers.9.mlp.down_proj.weight', 'model.language_model.layers.9.mlp.gate_proj.weight', 'model.language_model.layers.9.mlp.up_proj.weight', 'model.language_model.layers.9.post_attention_layernorm.weight', 'model.language_model.layers.9.self_attn.k_proj.bias', 'model.language_model.layers.9.self_attn.k_proj.weight', 'model.language_model.layers.9.self_attn.o_proj.weight', 'model.language_model.layers.9.self_attn.q_proj.bias', 'model.language_model.layers.9.self_attn.q_proj.weight', 'model.language_model.layers.9.self_attn.v_proj.bias', 'model.language_model.layers.9.self_attn.v_proj.weight', 'model.language_model.norm.weight', 'model.visual.blocks.0.attn.proj.bias', 'model.visual.blocks.0.attn.proj.weight', 'model.visual.blocks.0.attn.qkv.bias', 'model.visual.blocks.0.attn.qkv.weight', 'model.visual.blocks.0.mlp.fc1.bias', 'model.visual.blocks.0.mlp.fc1.weight', 'model.visual.blocks.0.mlp.fc2.bias', 'model.visual.blocks.0.mlp.fc2.weight', 'model.visual.blocks.0.norm1.bias', 'model.visual.blocks.0.norm1.weight', 'model.visual.blocks.0.norm2.bias', 'model.visual.blocks.0.norm2.weight', 'model.visual.blocks.1.attn.proj.bias', 'model.visual.blocks.1.attn.proj.weight', 'model.visual.blocks.1.attn.qkv.bias', 'model.visual.blocks.1.attn.qkv.weight', 'model.visual.blocks.1.mlp.fc1.bias', 'model.visual.blocks.1.mlp.fc1.weight', 'model.visual.blocks.1.mlp.fc2.bias', 'model.visual.blocks.1.mlp.fc2.weight', 'model.visual.blocks.1.norm1.bias', 'model.visual.blocks.1.norm1.weight', 'model.visual.blocks.1.norm2.bias', 'model.visual.blocks.1.norm2.weight', 'model.visual.blocks.10.attn.proj.bias', 'model.visual.blocks.10.attn.proj.weight', 'model.visual.blocks.10.attn.qkv.bias', 'model.visual.blocks.10.attn.qkv.weight', 'model.visual.blocks.10.mlp.fc1.bias', 'model.visual.blocks.10.mlp.fc1.weight', 'model.visual.blocks.10.mlp.fc2.bias', 'model.visual.blocks.10.mlp.fc2.weight', 'model.visual.blocks.10.norm1.bias', 'model.visual.blocks.10.norm1.weight', 'model.visual.blocks.10.norm2.bias', 'model.visual.blocks.10.norm2.weight', 'model.visual.blocks.11.attn.proj.bias', 'model.visual.blocks.11.attn.proj.weight', 'model.visual.blocks.11.attn.qkv.bias', 'model.visual.blocks.11.attn.qkv.weight', 'model.visual.blocks.11.mlp.fc1.bias', 'model.visual.blocks.11.mlp.fc1.weight', 'model.visual.blocks.11.mlp.fc2.bias', 'model.visual.blocks.11.mlp.fc2.weight', 'model.visual.blocks.11.norm1.bias', 'model.visual.blocks.11.norm1.weight', 'model.visual.blocks.11.norm2.bias', 'model.visual.blocks.11.norm2.weight', 'model.visual.blocks.12.attn.proj.bias', 'model.visual.blocks.12.attn.proj.weight', 'model.visual.blocks.12.attn.qkv.bias', 'model.visual.blocks.12.attn.qkv.weight', 'model.visual.blocks.12.mlp.fc1.bias', 'model.visual.blocks.12.mlp.fc1.weight', 'model.visual.blocks.12.mlp.fc2.bias', 'model.visual.blocks.12.mlp.fc2.weight', 'model.visual.blocks.12.norm1.bias', 'model.visual.blocks.12.norm1.weight', 'model.visual.blocks.12.norm2.bias', 'model.visual.blocks.12.norm2.weight', 'model.visual.blocks.13.attn.proj.bias', 'model.visual.blocks.13.attn.proj.weight', 'model.visual.blocks.13.attn.qkv.bias', 'model.visual.blocks.13.attn.qkv.weight', 'model.visual.blocks.13.mlp.fc1.bias', 'model.visual.blocks.13.mlp.fc1.weight', 'model.visual.blocks.13.mlp.fc2.bias', 'model.visual.blocks.13.mlp.fc2.weight', 'model.visual.blocks.13.norm1.bias', 'model.visual.blocks.13.norm1.weight', 'model.visual.blocks.13.norm2.bias', 'model.visual.blocks.13.norm2.weight', 'model.visual.blocks.14.attn.proj.bias', 'model.visual.blocks.14.attn.proj.weight', 'model.visual.blocks.14.attn.qkv.bias', 'model.visual.blocks.14.attn.qkv.weight', 'model.visual.blocks.14.mlp.fc1.bias', 'model.visual.blocks.14.mlp.fc1.weight', 'model.visual.blocks.14.mlp.fc2.bias', 'model.visual.blocks.14.mlp.fc2.weight', 'model.visual.blocks.14.norm1.bias', 'model.visual.blocks.14.norm1.weight', 'model.visual.blocks.14.norm2.bias', 'model.visual.blocks.14.norm2.weight', 'model.visual.blocks.15.attn.proj.bias', 'model.visual.blocks.15.attn.proj.weight', 'model.visual.blocks.15.attn.qkv.bias', 'model.visual.blocks.15.attn.qkv.weight', 'model.visual.blocks.15.mlp.fc1.bias', 'model.visual.blocks.15.mlp.fc1.weight', 'model.visual.blocks.15.mlp.fc2.bias', 'model.visual.blocks.15.mlp.fc2.weight', 'model.visual.blocks.15.norm1.bias', 'model.visual.blocks.15.norm1.weight', 'model.visual.blocks.15.norm2.bias', 'model.visual.blocks.15.norm2.weight', 'model.visual.blocks.16.attn.proj.bias', 'model.visual.blocks.16.attn.proj.weight', 'model.visual.blocks.16.attn.qkv.bias', 'model.visual.blocks.16.attn.qkv.weight', 'model.visual.blocks.16.mlp.fc1.bias', 'model.visual.blocks.16.mlp.fc1.weight', 'model.visual.blocks.16.mlp.fc2.bias', 'model.visual.blocks.16.mlp.fc2.weight', 'model.visual.blocks.16.norm1.bias', 'model.visual.blocks.16.norm1.weight', 'model.visual.blocks.16.norm2.bias', 'model.visual.blocks.16.norm2.weight', 'model.visual.blocks.17.attn.proj.bias', 'model.visual.blocks.17.attn.proj.weight', 'model.visual.blocks.17.attn.qkv.bias', 'model.visual.blocks.17.attn.qkv.weight', 'model.visual.blocks.17.mlp.fc1.bias', 'model.visual.blocks.17.mlp.fc1.weight', 'model.visual.blocks.17.mlp.fc2.bias', 'model.visual.blocks.17.mlp.fc2.weight', 'model.visual.blocks.17.norm1.bias', 'model.visual.blocks.17.norm1.weight', 'model.visual.blocks.17.norm2.bias', 'model.visual.blocks.17.norm2.weight', 'model.visual.blocks.18.attn.proj.bias', 'model.visual.blocks.18.attn.proj.weight', 'model.visual.blocks.18.attn.qkv.bias', 'model.visual.blocks.18.attn.qkv.weight', 'model.visual.blocks.18.mlp.fc1.bias', 'model.visual.blocks.18.mlp.fc1.weight', 'model.visual.blocks.18.mlp.fc2.bias', 'model.visual.blocks.18.mlp.fc2.weight', 'model.visual.blocks.18.norm1.bias', 'model.visual.blocks.18.norm1.weight', 'model.visual.blocks.18.norm2.bias', 'model.visual.blocks.18.norm2.weight', 'model.visual.blocks.19.attn.proj.bias', 'model.visual.blocks.19.attn.proj.weight', 'model.visual.blocks.19.attn.qkv.bias', 'model.visual.blocks.19.attn.qkv.weight', 'model.visual.blocks.19.mlp.fc1.bias', 'model.visual.blocks.19.mlp.fc1.weight', 'model.visual.blocks.19.mlp.fc2.bias', 'model.visual.blocks.19.mlp.fc2.weight', 'model.visual.blocks.19.norm1.bias', 'model.visual.blocks.19.norm1.weight', 'model.visual.blocks.19.norm2.bias', 'model.visual.blocks.19.norm2.weight', 'model.visual.blocks.2.attn.proj.bias', 'model.visual.blocks.2.attn.proj.weight', 'model.visual.blocks.2.attn.qkv.bias', 'model.visual.blocks.2.attn.qkv.weight', 'model.visual.blocks.2.mlp.fc1.bias', 'model.visual.blocks.2.mlp.fc1.weight', 'model.visual.blocks.2.mlp.fc2.bias', 'model.visual.blocks.2.mlp.fc2.weight', 'model.visual.blocks.2.norm1.bias', 'model.visual.blocks.2.norm1.weight', 'model.visual.blocks.2.norm2.bias', 'model.visual.blocks.2.norm2.weight', 'model.visual.blocks.20.attn.proj.bias', 'model.visual.blocks.20.attn.proj.weight', 'model.visual.blocks.20.attn.qkv.bias', 'model.visual.blocks.20.attn.qkv.weight', 'model.visual.blocks.20.mlp.fc1.bias', 'model.visual.blocks.20.mlp.fc1.weight', 'model.visual.blocks.20.mlp.fc2.bias', 'model.visual.blocks.20.mlp.fc2.weight', 'model.visual.blocks.20.norm1.bias', 'model.visual.blocks.20.norm1.weight', 'model.visual.blocks.20.norm2.bias', 'model.visual.blocks.20.norm2.weight', 'model.visual.blocks.21.attn.proj.bias', 'model.visual.blocks.21.attn.proj.weight', 'model.visual.blocks.21.attn.qkv.bias', 'model.visual.blocks.21.attn.qkv.weight', 'model.visual.blocks.21.mlp.fc1.bias', 'model.visual.blocks.21.mlp.fc1.weight', 'model.visual.blocks.21.mlp.fc2.bias', 'model.visual.blocks.21.mlp.fc2.weight', 'model.visual.blocks.21.norm1.bias', 'model.visual.blocks.21.norm1.weight', 'model.visual.blocks.21.norm2.bias', 'model.visual.blocks.21.norm2.weight', 'model.visual.blocks.22.attn.proj.bias', 'model.visual.blocks.22.attn.proj.weight', 'model.visual.blocks.22.attn.qkv.bias', 'model.visual.blocks.22.attn.qkv.weight', 'model.visual.blocks.22.mlp.fc1.bias', 'model.visual.blocks.22.mlp.fc1.weight', 'model.visual.blocks.22.mlp.fc2.bias', 'model.visual.blocks.22.mlp.fc2.weight', 'model.visual.blocks.22.norm1.bias', 'model.visual.blocks.22.norm1.weight', 'model.visual.blocks.22.norm2.bias', 'model.visual.blocks.22.norm2.weight', 'model.visual.blocks.23.attn.proj.bias', 'model.visual.blocks.23.attn.proj.weight', 'model.visual.blocks.23.attn.qkv.bias', 'model.visual.blocks.23.attn.qkv.weight', 'model.visual.blocks.23.mlp.fc1.bias', 'model.visual.blocks.23.mlp.fc1.weight', 'model.visual.blocks.23.mlp.fc2.bias', 'model.visual.blocks.23.mlp.fc2.weight', 'model.visual.blocks.23.norm1.bias', 'model.visual.blocks.23.norm1.weight', 'model.visual.blocks.23.norm2.bias', 'model.visual.blocks.23.norm2.weight', 'model.visual.blocks.24.attn.proj.bias', 'model.visual.blocks.24.attn.proj.weight', 'model.visual.blocks.24.attn.qkv.bias', 'model.visual.blocks.24.attn.qkv.weight', 'model.visual.blocks.24.mlp.fc1.bias', 'model.visual.blocks.24.mlp.fc1.weight', 'model.visual.blocks.24.mlp.fc2.bias', 'model.visual.blocks.24.mlp.fc2.weight', 'model.visual.blocks.24.norm1.bias', 'model.visual.blocks.24.norm1.weight', 'model.visual.blocks.24.norm2.bias', 'model.visual.blocks.24.norm2.weight', 'model.visual.blocks.25.attn.proj.bias', 'model.visual.blocks.25.attn.proj.weight', 'model.visual.blocks.25.attn.qkv.bias', 'model.visual.blocks.25.attn.qkv.weight', 'model.visual.blocks.25.mlp.fc1.bias', 'model.visual.blocks.25.mlp.fc1.weight', 'model.visual.blocks.25.mlp.fc2.bias', 'model.visual.blocks.25.mlp.fc2.weight', 'model.visual.blocks.25.norm1.bias', 'model.visual.blocks.25.norm1.weight', 'model.visual.blocks.25.norm2.bias', 'model.visual.blocks.25.norm2.weight', 'model.visual.blocks.26.attn.proj.bias', 'model.visual.blocks.26.attn.proj.weight', 'model.visual.blocks.26.attn.qkv.bias', 'model.visual.blocks.26.attn.qkv.weight', 'model.visual.blocks.26.mlp.fc1.bias', 'model.visual.blocks.26.mlp.fc1.weight', 'model.visual.blocks.26.mlp.fc2.bias', 'model.visual.blocks.26.mlp.fc2.weight', 'model.visual.blocks.26.norm1.bias', 'model.visual.blocks.26.norm1.weight', 'model.visual.blocks.26.norm2.bias', 'model.visual.blocks.26.norm2.weight', 'model.visual.blocks.27.attn.proj.bias', 'model.visual.blocks.27.attn.proj.weight', 'model.visual.blocks.27.attn.qkv.bias', 'model.visual.blocks.27.attn.qkv.weight', 'model.visual.blocks.27.mlp.fc1.bias', 'model.visual.blocks.27.mlp.fc1.weight', 'model.visual.blocks.27.mlp.fc2.bias', 'model.visual.blocks.27.mlp.fc2.weight', 'model.visual.blocks.27.norm1.bias', 'model.visual.blocks.27.norm1.weight', 'model.visual.blocks.27.norm2.bias', 'model.visual.blocks.27.norm2.weight', 'model.visual.blocks.28.attn.proj.bias', 'model.visual.blocks.28.attn.proj.weight', 'model.visual.blocks.28.attn.qkv.bias', 'model.visual.blocks.28.attn.qkv.weight', 'model.visual.blocks.28.mlp.fc1.bias', 'model.visual.blocks.28.mlp.fc1.weight', 'model.visual.blocks.28.mlp.fc2.bias', 'model.visual.blocks.28.mlp.fc2.weight', 'model.visual.blocks.28.norm1.bias', 'model.visual.blocks.28.norm1.weight', 'model.visual.blocks.28.norm2.bias', 'model.visual.blocks.28.norm2.weight', 'model.visual.blocks.29.attn.proj.bias', 'model.visual.blocks.29.attn.proj.weight', 'model.visual.blocks.29.attn.qkv.bias', 'model.visual.blocks.29.attn.qkv.weight', 'model.visual.blocks.29.mlp.fc1.bias', 'model.visual.blocks.29.mlp.fc1.weight', 'model.visual.blocks.29.mlp.fc2.bias', 'model.visual.blocks.29.mlp.fc2.weight', 'model.visual.blocks.29.norm1.bias', 'model.visual.blocks.29.norm1.weight', 'model.visual.blocks.29.norm2.bias', 'model.visual.blocks.29.norm2.weight', 'model.visual.blocks.3.attn.proj.bias', 'model.visual.blocks.3.attn.proj.weight', 'model.visual.blocks.3.attn.qkv.bias', 'model.visual.blocks.3.attn.qkv.weight', 'model.visual.blocks.3.mlp.fc1.bias', 'model.visual.blocks.3.mlp.fc1.weight', 'model.visual.blocks.3.mlp.fc2.bias', 'model.visual.blocks.3.mlp.fc2.weight', 'model.visual.blocks.3.norm1.bias', 'model.visual.blocks.3.norm1.weight', 'model.visual.blocks.3.norm2.bias', 'model.visual.blocks.3.norm2.weight', 'model.visual.blocks.30.attn.proj.bias', 'model.visual.blocks.30.attn.proj.weight', 'model.visual.blocks.30.attn.qkv.bias', 'model.visual.blocks.30.attn.qkv.weight', 'model.visual.blocks.30.mlp.fc1.bias', 'model.visual.blocks.30.mlp.fc1.weight', 'model.visual.blocks.30.mlp.fc2.bias', 'model.visual.blocks.30.mlp.fc2.weight', 'model.visual.blocks.30.norm1.bias', 'model.visual.blocks.30.norm1.weight', 'model.visual.blocks.30.norm2.bias', 'model.visual.blocks.30.norm2.weight', 'model.visual.blocks.31.attn.proj.bias', 'model.visual.blocks.31.attn.proj.weight', 'model.visual.blocks.31.attn.qkv.bias', 'model.visual.blocks.31.attn.qkv.weight', 'model.visual.blocks.31.mlp.fc1.bias', 'model.visual.blocks.31.mlp.fc1.weight', 'model.visual.blocks.31.mlp.fc2.bias', 'model.visual.blocks.31.mlp.fc2.weight', 'model.visual.blocks.31.norm1.bias', 'model.visual.blocks.31.norm1.weight', 'model.visual.blocks.31.norm2.bias', 'model.visual.blocks.31.norm2.weight', 'model.visual.blocks.4.attn.proj.bias', 'model.visual.blocks.4.attn.proj.weight', 'model.visual.blocks.4.attn.qkv.bias', 'model.visual.blocks.4.attn.qkv.weight', 'model.visual.blocks.4.mlp.fc1.bias', 'model.visual.blocks.4.mlp.fc1.weight', 'model.visual.blocks.4.mlp.fc2.bias', 'model.visual.blocks.4.mlp.fc2.weight', 'model.visual.blocks.4.norm1.bias', 'model.visual.blocks.4.norm1.weight', 'model.visual.blocks.4.norm2.bias', 'model.visual.blocks.4.norm2.weight', 'model.visual.blocks.5.attn.proj.bias', 'model.visual.blocks.5.attn.proj.weight', 'model.visual.blocks.5.attn.qkv.bias', 'model.visual.blocks.5.attn.qkv.weight', 'model.visual.blocks.5.mlp.fc1.bias', 'model.visual.blocks.5.mlp.fc1.weight', 'model.visual.blocks.5.mlp.fc2.bias', 'model.visual.blocks.5.mlp.fc2.weight', 'model.visual.blocks.5.norm1.bias', 'model.visual.blocks.5.norm1.weight', 'model.visual.blocks.5.norm2.bias', 'model.visual.blocks.5.norm2.weight', 'model.visual.blocks.6.attn.proj.bias', 'model.visual.blocks.6.attn.proj.weight', 'model.visual.blocks.6.attn.qkv.bias', 'model.visual.blocks.6.attn.qkv.weight', 'model.visual.blocks.6.mlp.fc1.bias', 'model.visual.blocks.6.mlp.fc1.weight', 'model.visual.blocks.6.mlp.fc2.bias', 'model.visual.blocks.6.mlp.fc2.weight', 'model.visual.blocks.6.norm1.bias', 'model.visual.blocks.6.norm1.weight', 'model.visual.blocks.6.norm2.bias', 'model.visual.blocks.6.norm2.weight', 'model.visual.blocks.7.attn.proj.bias', 'model.visual.blocks.7.attn.proj.weight', 'model.visual.blocks.7.attn.qkv.bias', 'model.visual.blocks.7.attn.qkv.weight', 'model.visual.blocks.7.mlp.fc1.bias', 'model.visual.blocks.7.mlp.fc1.weight', 'model.visual.blocks.7.mlp.fc2.bias', 'model.visual.blocks.7.mlp.fc2.weight', 'model.visual.blocks.7.norm1.bias', 'model.visual.blocks.7.norm1.weight', 'model.visual.blocks.7.norm2.bias', 'model.visual.blocks.7.norm2.weight', 'model.visual.blocks.8.attn.proj.bias', 'model.visual.blocks.8.attn.proj.weight', 'model.visual.blocks.8.attn.qkv.bias', 'model.visual.blocks.8.attn.qkv.weight', 'model.visual.blocks.8.mlp.fc1.bias', 'model.visual.blocks.8.mlp.fc1.weight', 'model.visual.blocks.8.mlp.fc2.bias', 'model.visual.blocks.8.mlp.fc2.weight', 'model.visual.blocks.8.norm1.bias', 'model.visual.blocks.8.norm1.weight', 'model.visual.blocks.8.norm2.bias', 'model.visual.blocks.8.norm2.weight', 'model.visual.blocks.9.attn.proj.bias', 'model.visual.blocks.9.attn.proj.weight', 'model.visual.blocks.9.attn.qkv.bias', 'model.visual.blocks.9.attn.qkv.weight', 'model.visual.blocks.9.mlp.fc1.bias', 'model.visual.blocks.9.mlp.fc1.weight', 'model.visual.blocks.9.mlp.fc2.bias', 'model.visual.blocks.9.mlp.fc2.weight', 'model.visual.blocks.9.norm1.bias', 'model.visual.blocks.9.norm1.weight', 'model.visual.blocks.9.norm2.bias', 'model.visual.blocks.9.norm2.weight', 'model.visual.merger.ln_q.bias', 'model.visual.merger.ln_q.weight', 'model.visual.merger.mlp.0.bias', 'model.visual.merger.mlp.0.weight', 'model.visual.merger.mlp.2.bias', 'model.visual.merger.mlp.2.weight', 'model.visual.patch_embed.proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57719f5aec0c455d86f9bf7f0417f998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/272 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5736aacd0e44979ce750fdf68aa4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a425c2333d2f4ac79261a6ad2ac10c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875f13025f1b4ef98fd5ab042889217e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f209c22645461aa4dd24a6c48d1cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b341dc1bda942019aaf12376570eace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95eea36d83f4c11afaea45415c10f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "processor = Qwen2VLProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11924a6f-98c9-4433-88e4-d47931b464e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "\n",
    "def generate_text_from_sample(model, processor, sample, max_new_tokens=1024, device=\"cuda\"):\n",
    "    # Prepare the text input by applying the chat template\n",
    "    text_input = processor.apply_chat_template(\n",
    "        sample[1:2], tokenize=False, add_generation_prompt=True  # Use the sample without the system message\n",
    "    )\n",
    "\n",
    "    # Process the visual input from the sample\n",
    "    image_inputs, _ = process_vision_info(sample)\n",
    "\n",
    "    # Prepare the inputs for the model\n",
    "    model_inputs = processor(\n",
    "        text=[text_input],\n",
    "        images=image_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\n",
    "        device\n",
    "    )  # Move inputs to the specified device\n",
    "\n",
    "    # Generate text with the model\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # Trim the generated ids to remove the input ids\n",
    "    trimmed_generated_ids = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "\n",
    "    # Decode the output text\n",
    "    output_text = processor.batch_decode(\n",
    "        trimmed_generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "\n",
    "    return output_text[0]  # Return the first decoded output text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4044fb75-b36c-45f9-accd-05f10a269c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU allocated memory: 0.00 GB\n",
      "GPU reserved memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import time\n",
    "\n",
    "\n",
    "def clear_memory():\n",
    "    # Delete variables if they exist in the current global scope\n",
    "    if \"inputs\" in globals():\n",
    "        del globals()[\"inputs\"]\n",
    "    if \"model\" in globals():\n",
    "        del globals()[\"model\"]\n",
    "    if \"processor\" in globals():\n",
    "        del globals()[\"processor\"]\n",
    "    if \"trainer\" in globals():\n",
    "        del globals()[\"trainer\"]\n",
    "    if \"peft_model\" in globals():\n",
    "        del globals()[\"peft_model\"]\n",
    "    if \"bnb_config\" in globals():\n",
    "        del globals()[\"bnb_config\"]\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Garbage collection and clearing CUDA memory\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(2)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8953d82f-12b3-4de3-8f93-20973b171cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4400cab52774f91b3f4be32abb8b7f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=bnb_config\n",
    ")\n",
    "processor = Qwen2VLProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef11814b-48ed-43b3-b9f7-54be6350729a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,089,536 || all params: 2,210,075,136 || trainable%: 0.0493\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply PEFT model adaptation\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2a59a0a-7319-4dce-a8cd-256c91c66b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "# Configure training arguments\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"output/qwen2\",  # Directory to save the model\n",
    "    num_train_epochs=24,  # Number of training epochs\n",
    "    per_device_train_batch_size=4,  # Batch size for training\n",
    "    per_device_eval_batch_size=4,  # Batch size for evaluation\n",
    "    gradient_accumulation_steps=8,  # Steps to accumulate gradients\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing for memory efficiency\n",
    "    # Optimizer and scheduler settings\n",
    "    optim=\"adamw_torch_fused\",  # Optimizer type\n",
    "    learning_rate=2e-4,  # Learning rate for training\n",
    "    lr_scheduler_type=\"constant\",  # Type of learning rate scheduler\n",
    "    # Logging and evaluation\n",
    "    logging_steps=10,  # Steps interval for logging\n",
    "    eval_steps=10,  # Steps interval for evaluation\n",
    "    eval_strategy=\"no\",  # Strategy for evaluation\n",
    "    save_strategy=\"no\",  # Strategy for saving the model\n",
    "    save_steps=20,  # Steps interval for saving\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to evaluate the best model\n",
    "    greater_is_better=False,  # Whether higher metric values are better\n",
    "    load_best_model_at_end=False,  # Load the best model after training\n",
    "    # Mixed precision and gradient settings\n",
    "    bf16=True,  # Use bfloat16 precision\n",
    "    tf32=True,  # Use TensorFloat-32 precision\n",
    "    max_grad_norm=0.3,  # Maximum norm for gradient clipping\n",
    "    warmup_ratio=0.03,  # Ratio of total steps for warmup\n",
    "    # Hub and reporting\n",
    "    push_to_hub=False,  # Whether to push model to Hugging Face Hub\n",
    "    report_to=\"none\",  # Reporting tool for tracking metrics\n",
    "    # Gradient checkpointing settings\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Options for gradient checkpointing\n",
    "    # Dataset configuration\n",
    "    dataset_text_field=\"\",  # Text field in dataset\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},  # Additional dataset options\n",
    "    # max_seq_length=1024  # Maximum sequence length for input\n",
    ")\n",
    "\n",
    "training_args.remove_unused_columns = False  # Keep unused columns in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a063706-7ba4-4cf4-98b8-974845b7aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data collator to encode text and image pairs\n",
    "def collate_fn(examples):\n",
    "    # Get the texts and images, and apply the chat template\n",
    "    texts = [\n",
    "        processor.apply_chat_template(example, tokenize=False) for example in examples\n",
    "    ]  # Prepare texts for processing\n",
    "    image_inputs = [process_vision_info(example)[0] for example in examples]  # Process the images to extract inputs\n",
    "\n",
    "    # Tokenize the texts and process the images\n",
    "    batch = processor(\n",
    "        text=texts, images=image_inputs, return_tensors=\"pt\", padding=True\n",
    "    )  # Encode texts and images into tensors\n",
    "\n",
    "    # The labels are the input_ids, and we mask the padding tokens in the loss computation\n",
    "    labels = batch[\"input_ids\"].clone()  # Clone input IDs for labels\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100  # Mask padding tokens in labels\n",
    "\n",
    "    # Ignore the image token index in the loss computation (model specific)\n",
    "    if isinstance(processor, Qwen2VLProcessor):  # Check if the processor is Qwen2VLProcessor\n",
    "        image_tokens = [151652, 151653, 151655]  # Specific image token IDs for Qwen2VLProcessor\n",
    "    else:\n",
    "        image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]  # Convert image token to ID\n",
    "\n",
    "    # Mask image token IDs in the labels\n",
    "    for image_token_id in image_tokens:\n",
    "        labels[labels == image_token_id] = -100  # Mask image token IDs in labels\n",
    "\n",
    "    batch[\"labels\"] = labels  # Add labels to the batch\n",
    "\n",
    "    return batch  # Return the prepared batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a394f448-d14f-445c-88bf-8e89c33bdf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=None,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor.tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b894af7e-dcf7-4327-bd27-4a94de549634",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacity of 23.48 GiB of which 5.12 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 14.55 GiB is allocated by PyTorch, and 3.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2239\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2237\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2240\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2554\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2547\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2548\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2550\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2552\u001b[0m )\n\u001b[1;32m   2553\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2554\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2557\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2558\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2559\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2560\u001b[0m ):\n\u001b[1;32m   2561\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2562\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3746\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3745\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3746\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3748\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3751\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3752\u001b[0m ):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:670\u001b[0m, in \u001b[0;36mSFTTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;124;03mCompute training loss and additionally compute token accuracies\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    669\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 670\u001b[0m (loss, outputs) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;66;03m# When using padding-free, the attention_mask is not present in the inputs, instead we have cu_seq_lens_q,\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;66;03m# cu_seq_lens_k, and max_length_k, max_length_q and position_ids.\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3811\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3809\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3810\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3811\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3812\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:814\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:802\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:43\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py:1757\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1756\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1757\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1768\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1770\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py:193\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:1746\u001b[0m, in \u001b[0;36mQwen2VLForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position)\u001b[0m\n\u001b[1;32m   1744\u001b[0m     \u001b[38;5;66;03m# Enable model parallelism\u001b[39;00m\n\u001b[1;32m   1745\u001b[0m     shift_labels \u001b[38;5;241m=\u001b[39m shift_labels\u001b[38;5;241m.\u001b[39mto(shift_logits\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1746\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshift_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1749\u001b[0m     output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacity of 23.48 GiB of which 5.12 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 14.55 GiB is allocated by PyTorch, and 3.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6a7cf58-c1c3-4b49-b459-b0f10b6d2f7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d10b5d7279449f6b7651b4a0f5c3b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Qwen/Qwen2-VL-2B-Instruct were not used when initializing Qwen2VLForConditionalGeneration: ['model.embed_tokens.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.9.self_attn.v_proj.weight', 'model.norm.weight', 'visual.blocks.0.attn.proj.bias', 'visual.blocks.0.attn.proj.weight', 'visual.blocks.0.attn.qkv.bias', 'visual.blocks.0.attn.qkv.weight', 'visual.blocks.0.mlp.fc1.bias', 'visual.blocks.0.mlp.fc1.weight', 'visual.blocks.0.mlp.fc2.bias', 'visual.blocks.0.mlp.fc2.weight', 'visual.blocks.0.norm1.bias', 'visual.blocks.0.norm1.weight', 'visual.blocks.0.norm2.bias', 'visual.blocks.0.norm2.weight', 'visual.blocks.1.attn.proj.bias', 'visual.blocks.1.attn.proj.weight', 'visual.blocks.1.attn.qkv.bias', 'visual.blocks.1.attn.qkv.weight', 'visual.blocks.1.mlp.fc1.bias', 'visual.blocks.1.mlp.fc1.weight', 'visual.blocks.1.mlp.fc2.bias', 'visual.blocks.1.mlp.fc2.weight', 'visual.blocks.1.norm1.bias', 'visual.blocks.1.norm1.weight', 'visual.blocks.1.norm2.bias', 'visual.blocks.1.norm2.weight', 'visual.blocks.10.attn.proj.bias', 'visual.blocks.10.attn.proj.weight', 'visual.blocks.10.attn.qkv.bias', 'visual.blocks.10.attn.qkv.weight', 'visual.blocks.10.mlp.fc1.bias', 'visual.blocks.10.mlp.fc1.weight', 'visual.blocks.10.mlp.fc2.bias', 'visual.blocks.10.mlp.fc2.weight', 'visual.blocks.10.norm1.bias', 'visual.blocks.10.norm1.weight', 'visual.blocks.10.norm2.bias', 'visual.blocks.10.norm2.weight', 'visual.blocks.11.attn.proj.bias', 'visual.blocks.11.attn.proj.weight', 'visual.blocks.11.attn.qkv.bias', 'visual.blocks.11.attn.qkv.weight', 'visual.blocks.11.mlp.fc1.bias', 'visual.blocks.11.mlp.fc1.weight', 'visual.blocks.11.mlp.fc2.bias', 'visual.blocks.11.mlp.fc2.weight', 'visual.blocks.11.norm1.bias', 'visual.blocks.11.norm1.weight', 'visual.blocks.11.norm2.bias', 'visual.blocks.11.norm2.weight', 'visual.blocks.12.attn.proj.bias', 'visual.blocks.12.attn.proj.weight', 'visual.blocks.12.attn.qkv.bias', 'visual.blocks.12.attn.qkv.weight', 'visual.blocks.12.mlp.fc1.bias', 'visual.blocks.12.mlp.fc1.weight', 'visual.blocks.12.mlp.fc2.bias', 'visual.blocks.12.mlp.fc2.weight', 'visual.blocks.12.norm1.bias', 'visual.blocks.12.norm1.weight', 'visual.blocks.12.norm2.bias', 'visual.blocks.12.norm2.weight', 'visual.blocks.13.attn.proj.bias', 'visual.blocks.13.attn.proj.weight', 'visual.blocks.13.attn.qkv.bias', 'visual.blocks.13.attn.qkv.weight', 'visual.blocks.13.mlp.fc1.bias', 'visual.blocks.13.mlp.fc1.weight', 'visual.blocks.13.mlp.fc2.bias', 'visual.blocks.13.mlp.fc2.weight', 'visual.blocks.13.norm1.bias', 'visual.blocks.13.norm1.weight', 'visual.blocks.13.norm2.bias', 'visual.blocks.13.norm2.weight', 'visual.blocks.14.attn.proj.bias', 'visual.blocks.14.attn.proj.weight', 'visual.blocks.14.attn.qkv.bias', 'visual.blocks.14.attn.qkv.weight', 'visual.blocks.14.mlp.fc1.bias', 'visual.blocks.14.mlp.fc1.weight', 'visual.blocks.14.mlp.fc2.bias', 'visual.blocks.14.mlp.fc2.weight', 'visual.blocks.14.norm1.bias', 'visual.blocks.14.norm1.weight', 'visual.blocks.14.norm2.bias', 'visual.blocks.14.norm2.weight', 'visual.blocks.15.attn.proj.bias', 'visual.blocks.15.attn.proj.weight', 'visual.blocks.15.attn.qkv.bias', 'visual.blocks.15.attn.qkv.weight', 'visual.blocks.15.mlp.fc1.bias', 'visual.blocks.15.mlp.fc1.weight', 'visual.blocks.15.mlp.fc2.bias', 'visual.blocks.15.mlp.fc2.weight', 'visual.blocks.15.norm1.bias', 'visual.blocks.15.norm1.weight', 'visual.blocks.15.norm2.bias', 'visual.blocks.15.norm2.weight', 'visual.blocks.16.attn.proj.bias', 'visual.blocks.16.attn.proj.weight', 'visual.blocks.16.attn.qkv.bias', 'visual.blocks.16.attn.qkv.weight', 'visual.blocks.16.mlp.fc1.bias', 'visual.blocks.16.mlp.fc1.weight', 'visual.blocks.16.mlp.fc2.bias', 'visual.blocks.16.mlp.fc2.weight', 'visual.blocks.16.norm1.bias', 'visual.blocks.16.norm1.weight', 'visual.blocks.16.norm2.bias', 'visual.blocks.16.norm2.weight', 'visual.blocks.17.attn.proj.bias', 'visual.blocks.17.attn.proj.weight', 'visual.blocks.17.attn.qkv.bias', 'visual.blocks.17.attn.qkv.weight', 'visual.blocks.17.mlp.fc1.bias', 'visual.blocks.17.mlp.fc1.weight', 'visual.blocks.17.mlp.fc2.bias', 'visual.blocks.17.mlp.fc2.weight', 'visual.blocks.17.norm1.bias', 'visual.blocks.17.norm1.weight', 'visual.blocks.17.norm2.bias', 'visual.blocks.17.norm2.weight', 'visual.blocks.18.attn.proj.bias', 'visual.blocks.18.attn.proj.weight', 'visual.blocks.18.attn.qkv.bias', 'visual.blocks.18.attn.qkv.weight', 'visual.blocks.18.mlp.fc1.bias', 'visual.blocks.18.mlp.fc1.weight', 'visual.blocks.18.mlp.fc2.bias', 'visual.blocks.18.mlp.fc2.weight', 'visual.blocks.18.norm1.bias', 'visual.blocks.18.norm1.weight', 'visual.blocks.18.norm2.bias', 'visual.blocks.18.norm2.weight', 'visual.blocks.19.attn.proj.bias', 'visual.blocks.19.attn.proj.weight', 'visual.blocks.19.attn.qkv.bias', 'visual.blocks.19.attn.qkv.weight', 'visual.blocks.19.mlp.fc1.bias', 'visual.blocks.19.mlp.fc1.weight', 'visual.blocks.19.mlp.fc2.bias', 'visual.blocks.19.mlp.fc2.weight', 'visual.blocks.19.norm1.bias', 'visual.blocks.19.norm1.weight', 'visual.blocks.19.norm2.bias', 'visual.blocks.19.norm2.weight', 'visual.blocks.2.attn.proj.bias', 'visual.blocks.2.attn.proj.weight', 'visual.blocks.2.attn.qkv.bias', 'visual.blocks.2.attn.qkv.weight', 'visual.blocks.2.mlp.fc1.bias', 'visual.blocks.2.mlp.fc1.weight', 'visual.blocks.2.mlp.fc2.bias', 'visual.blocks.2.mlp.fc2.weight', 'visual.blocks.2.norm1.bias', 'visual.blocks.2.norm1.weight', 'visual.blocks.2.norm2.bias', 'visual.blocks.2.norm2.weight', 'visual.blocks.20.attn.proj.bias', 'visual.blocks.20.attn.proj.weight', 'visual.blocks.20.attn.qkv.bias', 'visual.blocks.20.attn.qkv.weight', 'visual.blocks.20.mlp.fc1.bias', 'visual.blocks.20.mlp.fc1.weight', 'visual.blocks.20.mlp.fc2.bias', 'visual.blocks.20.mlp.fc2.weight', 'visual.blocks.20.norm1.bias', 'visual.blocks.20.norm1.weight', 'visual.blocks.20.norm2.bias', 'visual.blocks.20.norm2.weight', 'visual.blocks.21.attn.proj.bias', 'visual.blocks.21.attn.proj.weight', 'visual.blocks.21.attn.qkv.bias', 'visual.blocks.21.attn.qkv.weight', 'visual.blocks.21.mlp.fc1.bias', 'visual.blocks.21.mlp.fc1.weight', 'visual.blocks.21.mlp.fc2.bias', 'visual.blocks.21.mlp.fc2.weight', 'visual.blocks.21.norm1.bias', 'visual.blocks.21.norm1.weight', 'visual.blocks.21.norm2.bias', 'visual.blocks.21.norm2.weight', 'visual.blocks.22.attn.proj.bias', 'visual.blocks.22.attn.proj.weight', 'visual.blocks.22.attn.qkv.bias', 'visual.blocks.22.attn.qkv.weight', 'visual.blocks.22.mlp.fc1.bias', 'visual.blocks.22.mlp.fc1.weight', 'visual.blocks.22.mlp.fc2.bias', 'visual.blocks.22.mlp.fc2.weight', 'visual.blocks.22.norm1.bias', 'visual.blocks.22.norm1.weight', 'visual.blocks.22.norm2.bias', 'visual.blocks.22.norm2.weight', 'visual.blocks.23.attn.proj.bias', 'visual.blocks.23.attn.proj.weight', 'visual.blocks.23.attn.qkv.bias', 'visual.blocks.23.attn.qkv.weight', 'visual.blocks.23.mlp.fc1.bias', 'visual.blocks.23.mlp.fc1.weight', 'visual.blocks.23.mlp.fc2.bias', 'visual.blocks.23.mlp.fc2.weight', 'visual.blocks.23.norm1.bias', 'visual.blocks.23.norm1.weight', 'visual.blocks.23.norm2.bias', 'visual.blocks.23.norm2.weight', 'visual.blocks.24.attn.proj.bias', 'visual.blocks.24.attn.proj.weight', 'visual.blocks.24.attn.qkv.bias', 'visual.blocks.24.attn.qkv.weight', 'visual.blocks.24.mlp.fc1.bias', 'visual.blocks.24.mlp.fc1.weight', 'visual.blocks.24.mlp.fc2.bias', 'visual.blocks.24.mlp.fc2.weight', 'visual.blocks.24.norm1.bias', 'visual.blocks.24.norm1.weight', 'visual.blocks.24.norm2.bias', 'visual.blocks.24.norm2.weight', 'visual.blocks.25.attn.proj.bias', 'visual.blocks.25.attn.proj.weight', 'visual.blocks.25.attn.qkv.bias', 'visual.blocks.25.attn.qkv.weight', 'visual.blocks.25.mlp.fc1.bias', 'visual.blocks.25.mlp.fc1.weight', 'visual.blocks.25.mlp.fc2.bias', 'visual.blocks.25.mlp.fc2.weight', 'visual.blocks.25.norm1.bias', 'visual.blocks.25.norm1.weight', 'visual.blocks.25.norm2.bias', 'visual.blocks.25.norm2.weight', 'visual.blocks.26.attn.proj.bias', 'visual.blocks.26.attn.proj.weight', 'visual.blocks.26.attn.qkv.bias', 'visual.blocks.26.attn.qkv.weight', 'visual.blocks.26.mlp.fc1.bias', 'visual.blocks.26.mlp.fc1.weight', 'visual.blocks.26.mlp.fc2.bias', 'visual.blocks.26.mlp.fc2.weight', 'visual.blocks.26.norm1.bias', 'visual.blocks.26.norm1.weight', 'visual.blocks.26.norm2.bias', 'visual.blocks.26.norm2.weight', 'visual.blocks.27.attn.proj.bias', 'visual.blocks.27.attn.proj.weight', 'visual.blocks.27.attn.qkv.bias', 'visual.blocks.27.attn.qkv.weight', 'visual.blocks.27.mlp.fc1.bias', 'visual.blocks.27.mlp.fc1.weight', 'visual.blocks.27.mlp.fc2.bias', 'visual.blocks.27.mlp.fc2.weight', 'visual.blocks.27.norm1.bias', 'visual.blocks.27.norm1.weight', 'visual.blocks.27.norm2.bias', 'visual.blocks.27.norm2.weight', 'visual.blocks.28.attn.proj.bias', 'visual.blocks.28.attn.proj.weight', 'visual.blocks.28.attn.qkv.bias', 'visual.blocks.28.attn.qkv.weight', 'visual.blocks.28.mlp.fc1.bias', 'visual.blocks.28.mlp.fc1.weight', 'visual.blocks.28.mlp.fc2.bias', 'visual.blocks.28.mlp.fc2.weight', 'visual.blocks.28.norm1.bias', 'visual.blocks.28.norm1.weight', 'visual.blocks.28.norm2.bias', 'visual.blocks.28.norm2.weight', 'visual.blocks.29.attn.proj.bias', 'visual.blocks.29.attn.proj.weight', 'visual.blocks.29.attn.qkv.bias', 'visual.blocks.29.attn.qkv.weight', 'visual.blocks.29.mlp.fc1.bias', 'visual.blocks.29.mlp.fc1.weight', 'visual.blocks.29.mlp.fc2.bias', 'visual.blocks.29.mlp.fc2.weight', 'visual.blocks.29.norm1.bias', 'visual.blocks.29.norm1.weight', 'visual.blocks.29.norm2.bias', 'visual.blocks.29.norm2.weight', 'visual.blocks.3.attn.proj.bias', 'visual.blocks.3.attn.proj.weight', 'visual.blocks.3.attn.qkv.bias', 'visual.blocks.3.attn.qkv.weight', 'visual.blocks.3.mlp.fc1.bias', 'visual.blocks.3.mlp.fc1.weight', 'visual.blocks.3.mlp.fc2.bias', 'visual.blocks.3.mlp.fc2.weight', 'visual.blocks.3.norm1.bias', 'visual.blocks.3.norm1.weight', 'visual.blocks.3.norm2.bias', 'visual.blocks.3.norm2.weight', 'visual.blocks.30.attn.proj.bias', 'visual.blocks.30.attn.proj.weight', 'visual.blocks.30.attn.qkv.bias', 'visual.blocks.30.attn.qkv.weight', 'visual.blocks.30.mlp.fc1.bias', 'visual.blocks.30.mlp.fc1.weight', 'visual.blocks.30.mlp.fc2.bias', 'visual.blocks.30.mlp.fc2.weight', 'visual.blocks.30.norm1.bias', 'visual.blocks.30.norm1.weight', 'visual.blocks.30.norm2.bias', 'visual.blocks.30.norm2.weight', 'visual.blocks.31.attn.proj.bias', 'visual.blocks.31.attn.proj.weight', 'visual.blocks.31.attn.qkv.bias', 'visual.blocks.31.attn.qkv.weight', 'visual.blocks.31.mlp.fc1.bias', 'visual.blocks.31.mlp.fc1.weight', 'visual.blocks.31.mlp.fc2.bias', 'visual.blocks.31.mlp.fc2.weight', 'visual.blocks.31.norm1.bias', 'visual.blocks.31.norm1.weight', 'visual.blocks.31.norm2.bias', 'visual.blocks.31.norm2.weight', 'visual.blocks.4.attn.proj.bias', 'visual.blocks.4.attn.proj.weight', 'visual.blocks.4.attn.qkv.bias', 'visual.blocks.4.attn.qkv.weight', 'visual.blocks.4.mlp.fc1.bias', 'visual.blocks.4.mlp.fc1.weight', 'visual.blocks.4.mlp.fc2.bias', 'visual.blocks.4.mlp.fc2.weight', 'visual.blocks.4.norm1.bias', 'visual.blocks.4.norm1.weight', 'visual.blocks.4.norm2.bias', 'visual.blocks.4.norm2.weight', 'visual.blocks.5.attn.proj.bias', 'visual.blocks.5.attn.proj.weight', 'visual.blocks.5.attn.qkv.bias', 'visual.blocks.5.attn.qkv.weight', 'visual.blocks.5.mlp.fc1.bias', 'visual.blocks.5.mlp.fc1.weight', 'visual.blocks.5.mlp.fc2.bias', 'visual.blocks.5.mlp.fc2.weight', 'visual.blocks.5.norm1.bias', 'visual.blocks.5.norm1.weight', 'visual.blocks.5.norm2.bias', 'visual.blocks.5.norm2.weight', 'visual.blocks.6.attn.proj.bias', 'visual.blocks.6.attn.proj.weight', 'visual.blocks.6.attn.qkv.bias', 'visual.blocks.6.attn.qkv.weight', 'visual.blocks.6.mlp.fc1.bias', 'visual.blocks.6.mlp.fc1.weight', 'visual.blocks.6.mlp.fc2.bias', 'visual.blocks.6.mlp.fc2.weight', 'visual.blocks.6.norm1.bias', 'visual.blocks.6.norm1.weight', 'visual.blocks.6.norm2.bias', 'visual.blocks.6.norm2.weight', 'visual.blocks.7.attn.proj.bias', 'visual.blocks.7.attn.proj.weight', 'visual.blocks.7.attn.qkv.bias', 'visual.blocks.7.attn.qkv.weight', 'visual.blocks.7.mlp.fc1.bias', 'visual.blocks.7.mlp.fc1.weight', 'visual.blocks.7.mlp.fc2.bias', 'visual.blocks.7.mlp.fc2.weight', 'visual.blocks.7.norm1.bias', 'visual.blocks.7.norm1.weight', 'visual.blocks.7.norm2.bias', 'visual.blocks.7.norm2.weight', 'visual.blocks.8.attn.proj.bias', 'visual.blocks.8.attn.proj.weight', 'visual.blocks.8.attn.qkv.bias', 'visual.blocks.8.attn.qkv.weight', 'visual.blocks.8.mlp.fc1.bias', 'visual.blocks.8.mlp.fc1.weight', 'visual.blocks.8.mlp.fc2.bias', 'visual.blocks.8.mlp.fc2.weight', 'visual.blocks.8.norm1.bias', 'visual.blocks.8.norm1.weight', 'visual.blocks.8.norm2.bias', 'visual.blocks.8.norm2.weight', 'visual.blocks.9.attn.proj.bias', 'visual.blocks.9.attn.proj.weight', 'visual.blocks.9.attn.qkv.bias', 'visual.blocks.9.attn.qkv.weight', 'visual.blocks.9.mlp.fc1.bias', 'visual.blocks.9.mlp.fc1.weight', 'visual.blocks.9.mlp.fc2.bias', 'visual.blocks.9.mlp.fc2.weight', 'visual.blocks.9.norm1.bias', 'visual.blocks.9.norm1.weight', 'visual.blocks.9.norm2.bias', 'visual.blocks.9.norm2.weight', 'visual.merger.ln_q.bias', 'visual.merger.ln_q.weight', 'visual.merger.mlp.0.bias', 'visual.merger.mlp.0.weight', 'visual.merger.mlp.2.bias', 'visual.merger.mlp.2.weight', 'visual.patch_embed.proj.weight']\n",
      "- This IS expected if you are initializing Qwen2VLForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Qwen2VLForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Qwen2VLForConditionalGeneration were not initialized from the model checkpoint at Qwen/Qwen2-VL-2B-Instruct and are newly initialized: ['lm_head.weight', 'model.language_model.embed_tokens.weight', 'model.language_model.layers.0.input_layernorm.weight', 'model.language_model.layers.0.mlp.down_proj.weight', 'model.language_model.layers.0.mlp.gate_proj.weight', 'model.language_model.layers.0.mlp.up_proj.weight', 'model.language_model.layers.0.post_attention_layernorm.weight', 'model.language_model.layers.0.self_attn.k_proj.bias', 'model.language_model.layers.0.self_attn.k_proj.weight', 'model.language_model.layers.0.self_attn.o_proj.weight', 'model.language_model.layers.0.self_attn.q_proj.bias', 'model.language_model.layers.0.self_attn.q_proj.weight', 'model.language_model.layers.0.self_attn.v_proj.bias', 'model.language_model.layers.0.self_attn.v_proj.weight', 'model.language_model.layers.1.input_layernorm.weight', 'model.language_model.layers.1.mlp.down_proj.weight', 'model.language_model.layers.1.mlp.gate_proj.weight', 'model.language_model.layers.1.mlp.up_proj.weight', 'model.language_model.layers.1.post_attention_layernorm.weight', 'model.language_model.layers.1.self_attn.k_proj.bias', 'model.language_model.layers.1.self_attn.k_proj.weight', 'model.language_model.layers.1.self_attn.o_proj.weight', 'model.language_model.layers.1.self_attn.q_proj.bias', 'model.language_model.layers.1.self_attn.q_proj.weight', 'model.language_model.layers.1.self_attn.v_proj.bias', 'model.language_model.layers.1.self_attn.v_proj.weight', 'model.language_model.layers.10.input_layernorm.weight', 'model.language_model.layers.10.mlp.down_proj.weight', 'model.language_model.layers.10.mlp.gate_proj.weight', 'model.language_model.layers.10.mlp.up_proj.weight', 'model.language_model.layers.10.post_attention_layernorm.weight', 'model.language_model.layers.10.self_attn.k_proj.bias', 'model.language_model.layers.10.self_attn.k_proj.weight', 'model.language_model.layers.10.self_attn.o_proj.weight', 'model.language_model.layers.10.self_attn.q_proj.bias', 'model.language_model.layers.10.self_attn.q_proj.weight', 'model.language_model.layers.10.self_attn.v_proj.bias', 'model.language_model.layers.10.self_attn.v_proj.weight', 'model.language_model.layers.11.input_layernorm.weight', 'model.language_model.layers.11.mlp.down_proj.weight', 'model.language_model.layers.11.mlp.gate_proj.weight', 'model.language_model.layers.11.mlp.up_proj.weight', 'model.language_model.layers.11.post_attention_layernorm.weight', 'model.language_model.layers.11.self_attn.k_proj.bias', 'model.language_model.layers.11.self_attn.k_proj.weight', 'model.language_model.layers.11.self_attn.o_proj.weight', 'model.language_model.layers.11.self_attn.q_proj.bias', 'model.language_model.layers.11.self_attn.q_proj.weight', 'model.language_model.layers.11.self_attn.v_proj.bias', 'model.language_model.layers.11.self_attn.v_proj.weight', 'model.language_model.layers.12.input_layernorm.weight', 'model.language_model.layers.12.mlp.down_proj.weight', 'model.language_model.layers.12.mlp.gate_proj.weight', 'model.language_model.layers.12.mlp.up_proj.weight', 'model.language_model.layers.12.post_attention_layernorm.weight', 'model.language_model.layers.12.self_attn.k_proj.bias', 'model.language_model.layers.12.self_attn.k_proj.weight', 'model.language_model.layers.12.self_attn.o_proj.weight', 'model.language_model.layers.12.self_attn.q_proj.bias', 'model.language_model.layers.12.self_attn.q_proj.weight', 'model.language_model.layers.12.self_attn.v_proj.bias', 'model.language_model.layers.12.self_attn.v_proj.weight', 'model.language_model.layers.13.input_layernorm.weight', 'model.language_model.layers.13.mlp.down_proj.weight', 'model.language_model.layers.13.mlp.gate_proj.weight', 'model.language_model.layers.13.mlp.up_proj.weight', 'model.language_model.layers.13.post_attention_layernorm.weight', 'model.language_model.layers.13.self_attn.k_proj.bias', 'model.language_model.layers.13.self_attn.k_proj.weight', 'model.language_model.layers.13.self_attn.o_proj.weight', 'model.language_model.layers.13.self_attn.q_proj.bias', 'model.language_model.layers.13.self_attn.q_proj.weight', 'model.language_model.layers.13.self_attn.v_proj.bias', 'model.language_model.layers.13.self_attn.v_proj.weight', 'model.language_model.layers.14.input_layernorm.weight', 'model.language_model.layers.14.mlp.down_proj.weight', 'model.language_model.layers.14.mlp.gate_proj.weight', 'model.language_model.layers.14.mlp.up_proj.weight', 'model.language_model.layers.14.post_attention_layernorm.weight', 'model.language_model.layers.14.self_attn.k_proj.bias', 'model.language_model.layers.14.self_attn.k_proj.weight', 'model.language_model.layers.14.self_attn.o_proj.weight', 'model.language_model.layers.14.self_attn.q_proj.bias', 'model.language_model.layers.14.self_attn.q_proj.weight', 'model.language_model.layers.14.self_attn.v_proj.bias', 'model.language_model.layers.14.self_attn.v_proj.weight', 'model.language_model.layers.15.input_layernorm.weight', 'model.language_model.layers.15.mlp.down_proj.weight', 'model.language_model.layers.15.mlp.gate_proj.weight', 'model.language_model.layers.15.mlp.up_proj.weight', 'model.language_model.layers.15.post_attention_layernorm.weight', 'model.language_model.layers.15.self_attn.k_proj.bias', 'model.language_model.layers.15.self_attn.k_proj.weight', 'model.language_model.layers.15.self_attn.o_proj.weight', 'model.language_model.layers.15.self_attn.q_proj.bias', 'model.language_model.layers.15.self_attn.q_proj.weight', 'model.language_model.layers.15.self_attn.v_proj.bias', 'model.language_model.layers.15.self_attn.v_proj.weight', 'model.language_model.layers.16.input_layernorm.weight', 'model.language_model.layers.16.mlp.down_proj.weight', 'model.language_model.layers.16.mlp.gate_proj.weight', 'model.language_model.layers.16.mlp.up_proj.weight', 'model.language_model.layers.16.post_attention_layernorm.weight', 'model.language_model.layers.16.self_attn.k_proj.bias', 'model.language_model.layers.16.self_attn.k_proj.weight', 'model.language_model.layers.16.self_attn.o_proj.weight', 'model.language_model.layers.16.self_attn.q_proj.bias', 'model.language_model.layers.16.self_attn.q_proj.weight', 'model.language_model.layers.16.self_attn.v_proj.bias', 'model.language_model.layers.16.self_attn.v_proj.weight', 'model.language_model.layers.17.input_layernorm.weight', 'model.language_model.layers.17.mlp.down_proj.weight', 'model.language_model.layers.17.mlp.gate_proj.weight', 'model.language_model.layers.17.mlp.up_proj.weight', 'model.language_model.layers.17.post_attention_layernorm.weight', 'model.language_model.layers.17.self_attn.k_proj.bias', 'model.language_model.layers.17.self_attn.k_proj.weight', 'model.language_model.layers.17.self_attn.o_proj.weight', 'model.language_model.layers.17.self_attn.q_proj.bias', 'model.language_model.layers.17.self_attn.q_proj.weight', 'model.language_model.layers.17.self_attn.v_proj.bias', 'model.language_model.layers.17.self_attn.v_proj.weight', 'model.language_model.layers.18.input_layernorm.weight', 'model.language_model.layers.18.mlp.down_proj.weight', 'model.language_model.layers.18.mlp.gate_proj.weight', 'model.language_model.layers.18.mlp.up_proj.weight', 'model.language_model.layers.18.post_attention_layernorm.weight', 'model.language_model.layers.18.self_attn.k_proj.bias', 'model.language_model.layers.18.self_attn.k_proj.weight', 'model.language_model.layers.18.self_attn.o_proj.weight', 'model.language_model.layers.18.self_attn.q_proj.bias', 'model.language_model.layers.18.self_attn.q_proj.weight', 'model.language_model.layers.18.self_attn.v_proj.bias', 'model.language_model.layers.18.self_attn.v_proj.weight', 'model.language_model.layers.19.input_layernorm.weight', 'model.language_model.layers.19.mlp.down_proj.weight', 'model.language_model.layers.19.mlp.gate_proj.weight', 'model.language_model.layers.19.mlp.up_proj.weight', 'model.language_model.layers.19.post_attention_layernorm.weight', 'model.language_model.layers.19.self_attn.k_proj.bias', 'model.language_model.layers.19.self_attn.k_proj.weight', 'model.language_model.layers.19.self_attn.o_proj.weight', 'model.language_model.layers.19.self_attn.q_proj.bias', 'model.language_model.layers.19.self_attn.q_proj.weight', 'model.language_model.layers.19.self_attn.v_proj.bias', 'model.language_model.layers.19.self_attn.v_proj.weight', 'model.language_model.layers.2.input_layernorm.weight', 'model.language_model.layers.2.mlp.down_proj.weight', 'model.language_model.layers.2.mlp.gate_proj.weight', 'model.language_model.layers.2.mlp.up_proj.weight', 'model.language_model.layers.2.post_attention_layernorm.weight', 'model.language_model.layers.2.self_attn.k_proj.bias', 'model.language_model.layers.2.self_attn.k_proj.weight', 'model.language_model.layers.2.self_attn.o_proj.weight', 'model.language_model.layers.2.self_attn.q_proj.bias', 'model.language_model.layers.2.self_attn.q_proj.weight', 'model.language_model.layers.2.self_attn.v_proj.bias', 'model.language_model.layers.2.self_attn.v_proj.weight', 'model.language_model.layers.20.input_layernorm.weight', 'model.language_model.layers.20.mlp.down_proj.weight', 'model.language_model.layers.20.mlp.gate_proj.weight', 'model.language_model.layers.20.mlp.up_proj.weight', 'model.language_model.layers.20.post_attention_layernorm.weight', 'model.language_model.layers.20.self_attn.k_proj.bias', 'model.language_model.layers.20.self_attn.k_proj.weight', 'model.language_model.layers.20.self_attn.o_proj.weight', 'model.language_model.layers.20.self_attn.q_proj.bias', 'model.language_model.layers.20.self_attn.q_proj.weight', 'model.language_model.layers.20.self_attn.v_proj.bias', 'model.language_model.layers.20.self_attn.v_proj.weight', 'model.language_model.layers.21.input_layernorm.weight', 'model.language_model.layers.21.mlp.down_proj.weight', 'model.language_model.layers.21.mlp.gate_proj.weight', 'model.language_model.layers.21.mlp.up_proj.weight', 'model.language_model.layers.21.post_attention_layernorm.weight', 'model.language_model.layers.21.self_attn.k_proj.bias', 'model.language_model.layers.21.self_attn.k_proj.weight', 'model.language_model.layers.21.self_attn.o_proj.weight', 'model.language_model.layers.21.self_attn.q_proj.bias', 'model.language_model.layers.21.self_attn.q_proj.weight', 'model.language_model.layers.21.self_attn.v_proj.bias', 'model.language_model.layers.21.self_attn.v_proj.weight', 'model.language_model.layers.22.input_layernorm.weight', 'model.language_model.layers.22.mlp.down_proj.weight', 'model.language_model.layers.22.mlp.gate_proj.weight', 'model.language_model.layers.22.mlp.up_proj.weight', 'model.language_model.layers.22.post_attention_layernorm.weight', 'model.language_model.layers.22.self_attn.k_proj.bias', 'model.language_model.layers.22.self_attn.k_proj.weight', 'model.language_model.layers.22.self_attn.o_proj.weight', 'model.language_model.layers.22.self_attn.q_proj.bias', 'model.language_model.layers.22.self_attn.q_proj.weight', 'model.language_model.layers.22.self_attn.v_proj.bias', 'model.language_model.layers.22.self_attn.v_proj.weight', 'model.language_model.layers.23.input_layernorm.weight', 'model.language_model.layers.23.mlp.down_proj.weight', 'model.language_model.layers.23.mlp.gate_proj.weight', 'model.language_model.layers.23.mlp.up_proj.weight', 'model.language_model.layers.23.post_attention_layernorm.weight', 'model.language_model.layers.23.self_attn.k_proj.bias', 'model.language_model.layers.23.self_attn.k_proj.weight', 'model.language_model.layers.23.self_attn.o_proj.weight', 'model.language_model.layers.23.self_attn.q_proj.bias', 'model.language_model.layers.23.self_attn.q_proj.weight', 'model.language_model.layers.23.self_attn.v_proj.bias', 'model.language_model.layers.23.self_attn.v_proj.weight', 'model.language_model.layers.24.input_layernorm.weight', 'model.language_model.layers.24.mlp.down_proj.weight', 'model.language_model.layers.24.mlp.gate_proj.weight', 'model.language_model.layers.24.mlp.up_proj.weight', 'model.language_model.layers.24.post_attention_layernorm.weight', 'model.language_model.layers.24.self_attn.k_proj.bias', 'model.language_model.layers.24.self_attn.k_proj.weight', 'model.language_model.layers.24.self_attn.o_proj.weight', 'model.language_model.layers.24.self_attn.q_proj.bias', 'model.language_model.layers.24.self_attn.q_proj.weight', 'model.language_model.layers.24.self_attn.v_proj.bias', 'model.language_model.layers.24.self_attn.v_proj.weight', 'model.language_model.layers.25.input_layernorm.weight', 'model.language_model.layers.25.mlp.down_proj.weight', 'model.language_model.layers.25.mlp.gate_proj.weight', 'model.language_model.layers.25.mlp.up_proj.weight', 'model.language_model.layers.25.post_attention_layernorm.weight', 'model.language_model.layers.25.self_attn.k_proj.bias', 'model.language_model.layers.25.self_attn.k_proj.weight', 'model.language_model.layers.25.self_attn.o_proj.weight', 'model.language_model.layers.25.self_attn.q_proj.bias', 'model.language_model.layers.25.self_attn.q_proj.weight', 'model.language_model.layers.25.self_attn.v_proj.bias', 'model.language_model.layers.25.self_attn.v_proj.weight', 'model.language_model.layers.26.input_layernorm.weight', 'model.language_model.layers.26.mlp.down_proj.weight', 'model.language_model.layers.26.mlp.gate_proj.weight', 'model.language_model.layers.26.mlp.up_proj.weight', 'model.language_model.layers.26.post_attention_layernorm.weight', 'model.language_model.layers.26.self_attn.k_proj.bias', 'model.language_model.layers.26.self_attn.k_proj.weight', 'model.language_model.layers.26.self_attn.o_proj.weight', 'model.language_model.layers.26.self_attn.q_proj.bias', 'model.language_model.layers.26.self_attn.q_proj.weight', 'model.language_model.layers.26.self_attn.v_proj.bias', 'model.language_model.layers.26.self_attn.v_proj.weight', 'model.language_model.layers.27.input_layernorm.weight', 'model.language_model.layers.27.mlp.down_proj.weight', 'model.language_model.layers.27.mlp.gate_proj.weight', 'model.language_model.layers.27.mlp.up_proj.weight', 'model.language_model.layers.27.post_attention_layernorm.weight', 'model.language_model.layers.27.self_attn.k_proj.bias', 'model.language_model.layers.27.self_attn.k_proj.weight', 'model.language_model.layers.27.self_attn.o_proj.weight', 'model.language_model.layers.27.self_attn.q_proj.bias', 'model.language_model.layers.27.self_attn.q_proj.weight', 'model.language_model.layers.27.self_attn.v_proj.bias', 'model.language_model.layers.27.self_attn.v_proj.weight', 'model.language_model.layers.3.input_layernorm.weight', 'model.language_model.layers.3.mlp.down_proj.weight', 'model.language_model.layers.3.mlp.gate_proj.weight', 'model.language_model.layers.3.mlp.up_proj.weight', 'model.language_model.layers.3.post_attention_layernorm.weight', 'model.language_model.layers.3.self_attn.k_proj.bias', 'model.language_model.layers.3.self_attn.k_proj.weight', 'model.language_model.layers.3.self_attn.o_proj.weight', 'model.language_model.layers.3.self_attn.q_proj.bias', 'model.language_model.layers.3.self_attn.q_proj.weight', 'model.language_model.layers.3.self_attn.v_proj.bias', 'model.language_model.layers.3.self_attn.v_proj.weight', 'model.language_model.layers.4.input_layernorm.weight', 'model.language_model.layers.4.mlp.down_proj.weight', 'model.language_model.layers.4.mlp.gate_proj.weight', 'model.language_model.layers.4.mlp.up_proj.weight', 'model.language_model.layers.4.post_attention_layernorm.weight', 'model.language_model.layers.4.self_attn.k_proj.bias', 'model.language_model.layers.4.self_attn.k_proj.weight', 'model.language_model.layers.4.self_attn.o_proj.weight', 'model.language_model.layers.4.self_attn.q_proj.bias', 'model.language_model.layers.4.self_attn.q_proj.weight', 'model.language_model.layers.4.self_attn.v_proj.bias', 'model.language_model.layers.4.self_attn.v_proj.weight', 'model.language_model.layers.5.input_layernorm.weight', 'model.language_model.layers.5.mlp.down_proj.weight', 'model.language_model.layers.5.mlp.gate_proj.weight', 'model.language_model.layers.5.mlp.up_proj.weight', 'model.language_model.layers.5.post_attention_layernorm.weight', 'model.language_model.layers.5.self_attn.k_proj.bias', 'model.language_model.layers.5.self_attn.k_proj.weight', 'model.language_model.layers.5.self_attn.o_proj.weight', 'model.language_model.layers.5.self_attn.q_proj.bias', 'model.language_model.layers.5.self_attn.q_proj.weight', 'model.language_model.layers.5.self_attn.v_proj.bias', 'model.language_model.layers.5.self_attn.v_proj.weight', 'model.language_model.layers.6.input_layernorm.weight', 'model.language_model.layers.6.mlp.down_proj.weight', 'model.language_model.layers.6.mlp.gate_proj.weight', 'model.language_model.layers.6.mlp.up_proj.weight', 'model.language_model.layers.6.post_attention_layernorm.weight', 'model.language_model.layers.6.self_attn.k_proj.bias', 'model.language_model.layers.6.self_attn.k_proj.weight', 'model.language_model.layers.6.self_attn.o_proj.weight', 'model.language_model.layers.6.self_attn.q_proj.bias', 'model.language_model.layers.6.self_attn.q_proj.weight', 'model.language_model.layers.6.self_attn.v_proj.bias', 'model.language_model.layers.6.self_attn.v_proj.weight', 'model.language_model.layers.7.input_layernorm.weight', 'model.language_model.layers.7.mlp.down_proj.weight', 'model.language_model.layers.7.mlp.gate_proj.weight', 'model.language_model.layers.7.mlp.up_proj.weight', 'model.language_model.layers.7.post_attention_layernorm.weight', 'model.language_model.layers.7.self_attn.k_proj.bias', 'model.language_model.layers.7.self_attn.k_proj.weight', 'model.language_model.layers.7.self_attn.o_proj.weight', 'model.language_model.layers.7.self_attn.q_proj.bias', 'model.language_model.layers.7.self_attn.q_proj.weight', 'model.language_model.layers.7.self_attn.v_proj.bias', 'model.language_model.layers.7.self_attn.v_proj.weight', 'model.language_model.layers.8.input_layernorm.weight', 'model.language_model.layers.8.mlp.down_proj.weight', 'model.language_model.layers.8.mlp.gate_proj.weight', 'model.language_model.layers.8.mlp.up_proj.weight', 'model.language_model.layers.8.post_attention_layernorm.weight', 'model.language_model.layers.8.self_attn.k_proj.bias', 'model.language_model.layers.8.self_attn.k_proj.weight', 'model.language_model.layers.8.self_attn.o_proj.weight', 'model.language_model.layers.8.self_attn.q_proj.bias', 'model.language_model.layers.8.self_attn.q_proj.weight', 'model.language_model.layers.8.self_attn.v_proj.bias', 'model.language_model.layers.8.self_attn.v_proj.weight', 'model.language_model.layers.9.input_layernorm.weight', 'model.language_model.layers.9.mlp.down_proj.weight', 'model.language_model.layers.9.mlp.gate_proj.weight', 'model.language_model.layers.9.mlp.up_proj.weight', 'model.language_model.layers.9.post_attention_layernorm.weight', 'model.language_model.layers.9.self_attn.k_proj.bias', 'model.language_model.layers.9.self_attn.k_proj.weight', 'model.language_model.layers.9.self_attn.o_proj.weight', 'model.language_model.layers.9.self_attn.q_proj.bias', 'model.language_model.layers.9.self_attn.q_proj.weight', 'model.language_model.layers.9.self_attn.v_proj.bias', 'model.language_model.layers.9.self_attn.v_proj.weight', 'model.language_model.norm.weight', 'model.visual.blocks.0.attn.proj.bias', 'model.visual.blocks.0.attn.proj.weight', 'model.visual.blocks.0.attn.qkv.bias', 'model.visual.blocks.0.attn.qkv.weight', 'model.visual.blocks.0.mlp.fc1.bias', 'model.visual.blocks.0.mlp.fc1.weight', 'model.visual.blocks.0.mlp.fc2.bias', 'model.visual.blocks.0.mlp.fc2.weight', 'model.visual.blocks.0.norm1.bias', 'model.visual.blocks.0.norm1.weight', 'model.visual.blocks.0.norm2.bias', 'model.visual.blocks.0.norm2.weight', 'model.visual.blocks.1.attn.proj.bias', 'model.visual.blocks.1.attn.proj.weight', 'model.visual.blocks.1.attn.qkv.bias', 'model.visual.blocks.1.attn.qkv.weight', 'model.visual.blocks.1.mlp.fc1.bias', 'model.visual.blocks.1.mlp.fc1.weight', 'model.visual.blocks.1.mlp.fc2.bias', 'model.visual.blocks.1.mlp.fc2.weight', 'model.visual.blocks.1.norm1.bias', 'model.visual.blocks.1.norm1.weight', 'model.visual.blocks.1.norm2.bias', 'model.visual.blocks.1.norm2.weight', 'model.visual.blocks.10.attn.proj.bias', 'model.visual.blocks.10.attn.proj.weight', 'model.visual.blocks.10.attn.qkv.bias', 'model.visual.blocks.10.attn.qkv.weight', 'model.visual.blocks.10.mlp.fc1.bias', 'model.visual.blocks.10.mlp.fc1.weight', 'model.visual.blocks.10.mlp.fc2.bias', 'model.visual.blocks.10.mlp.fc2.weight', 'model.visual.blocks.10.norm1.bias', 'model.visual.blocks.10.norm1.weight', 'model.visual.blocks.10.norm2.bias', 'model.visual.blocks.10.norm2.weight', 'model.visual.blocks.11.attn.proj.bias', 'model.visual.blocks.11.attn.proj.weight', 'model.visual.blocks.11.attn.qkv.bias', 'model.visual.blocks.11.attn.qkv.weight', 'model.visual.blocks.11.mlp.fc1.bias', 'model.visual.blocks.11.mlp.fc1.weight', 'model.visual.blocks.11.mlp.fc2.bias', 'model.visual.blocks.11.mlp.fc2.weight', 'model.visual.blocks.11.norm1.bias', 'model.visual.blocks.11.norm1.weight', 'model.visual.blocks.11.norm2.bias', 'model.visual.blocks.11.norm2.weight', 'model.visual.blocks.12.attn.proj.bias', 'model.visual.blocks.12.attn.proj.weight', 'model.visual.blocks.12.attn.qkv.bias', 'model.visual.blocks.12.attn.qkv.weight', 'model.visual.blocks.12.mlp.fc1.bias', 'model.visual.blocks.12.mlp.fc1.weight', 'model.visual.blocks.12.mlp.fc2.bias', 'model.visual.blocks.12.mlp.fc2.weight', 'model.visual.blocks.12.norm1.bias', 'model.visual.blocks.12.norm1.weight', 'model.visual.blocks.12.norm2.bias', 'model.visual.blocks.12.norm2.weight', 'model.visual.blocks.13.attn.proj.bias', 'model.visual.blocks.13.attn.proj.weight', 'model.visual.blocks.13.attn.qkv.bias', 'model.visual.blocks.13.attn.qkv.weight', 'model.visual.blocks.13.mlp.fc1.bias', 'model.visual.blocks.13.mlp.fc1.weight', 'model.visual.blocks.13.mlp.fc2.bias', 'model.visual.blocks.13.mlp.fc2.weight', 'model.visual.blocks.13.norm1.bias', 'model.visual.blocks.13.norm1.weight', 'model.visual.blocks.13.norm2.bias', 'model.visual.blocks.13.norm2.weight', 'model.visual.blocks.14.attn.proj.bias', 'model.visual.blocks.14.attn.proj.weight', 'model.visual.blocks.14.attn.qkv.bias', 'model.visual.blocks.14.attn.qkv.weight', 'model.visual.blocks.14.mlp.fc1.bias', 'model.visual.blocks.14.mlp.fc1.weight', 'model.visual.blocks.14.mlp.fc2.bias', 'model.visual.blocks.14.mlp.fc2.weight', 'model.visual.blocks.14.norm1.bias', 'model.visual.blocks.14.norm1.weight', 'model.visual.blocks.14.norm2.bias', 'model.visual.blocks.14.norm2.weight', 'model.visual.blocks.15.attn.proj.bias', 'model.visual.blocks.15.attn.proj.weight', 'model.visual.blocks.15.attn.qkv.bias', 'model.visual.blocks.15.attn.qkv.weight', 'model.visual.blocks.15.mlp.fc1.bias', 'model.visual.blocks.15.mlp.fc1.weight', 'model.visual.blocks.15.mlp.fc2.bias', 'model.visual.blocks.15.mlp.fc2.weight', 'model.visual.blocks.15.norm1.bias', 'model.visual.blocks.15.norm1.weight', 'model.visual.blocks.15.norm2.bias', 'model.visual.blocks.15.norm2.weight', 'model.visual.blocks.16.attn.proj.bias', 'model.visual.blocks.16.attn.proj.weight', 'model.visual.blocks.16.attn.qkv.bias', 'model.visual.blocks.16.attn.qkv.weight', 'model.visual.blocks.16.mlp.fc1.bias', 'model.visual.blocks.16.mlp.fc1.weight', 'model.visual.blocks.16.mlp.fc2.bias', 'model.visual.blocks.16.mlp.fc2.weight', 'model.visual.blocks.16.norm1.bias', 'model.visual.blocks.16.norm1.weight', 'model.visual.blocks.16.norm2.bias', 'model.visual.blocks.16.norm2.weight', 'model.visual.blocks.17.attn.proj.bias', 'model.visual.blocks.17.attn.proj.weight', 'model.visual.blocks.17.attn.qkv.bias', 'model.visual.blocks.17.attn.qkv.weight', 'model.visual.blocks.17.mlp.fc1.bias', 'model.visual.blocks.17.mlp.fc1.weight', 'model.visual.blocks.17.mlp.fc2.bias', 'model.visual.blocks.17.mlp.fc2.weight', 'model.visual.blocks.17.norm1.bias', 'model.visual.blocks.17.norm1.weight', 'model.visual.blocks.17.norm2.bias', 'model.visual.blocks.17.norm2.weight', 'model.visual.blocks.18.attn.proj.bias', 'model.visual.blocks.18.attn.proj.weight', 'model.visual.blocks.18.attn.qkv.bias', 'model.visual.blocks.18.attn.qkv.weight', 'model.visual.blocks.18.mlp.fc1.bias', 'model.visual.blocks.18.mlp.fc1.weight', 'model.visual.blocks.18.mlp.fc2.bias', 'model.visual.blocks.18.mlp.fc2.weight', 'model.visual.blocks.18.norm1.bias', 'model.visual.blocks.18.norm1.weight', 'model.visual.blocks.18.norm2.bias', 'model.visual.blocks.18.norm2.weight', 'model.visual.blocks.19.attn.proj.bias', 'model.visual.blocks.19.attn.proj.weight', 'model.visual.blocks.19.attn.qkv.bias', 'model.visual.blocks.19.attn.qkv.weight', 'model.visual.blocks.19.mlp.fc1.bias', 'model.visual.blocks.19.mlp.fc1.weight', 'model.visual.blocks.19.mlp.fc2.bias', 'model.visual.blocks.19.mlp.fc2.weight', 'model.visual.blocks.19.norm1.bias', 'model.visual.blocks.19.norm1.weight', 'model.visual.blocks.19.norm2.bias', 'model.visual.blocks.19.norm2.weight', 'model.visual.blocks.2.attn.proj.bias', 'model.visual.blocks.2.attn.proj.weight', 'model.visual.blocks.2.attn.qkv.bias', 'model.visual.blocks.2.attn.qkv.weight', 'model.visual.blocks.2.mlp.fc1.bias', 'model.visual.blocks.2.mlp.fc1.weight', 'model.visual.blocks.2.mlp.fc2.bias', 'model.visual.blocks.2.mlp.fc2.weight', 'model.visual.blocks.2.norm1.bias', 'model.visual.blocks.2.norm1.weight', 'model.visual.blocks.2.norm2.bias', 'model.visual.blocks.2.norm2.weight', 'model.visual.blocks.20.attn.proj.bias', 'model.visual.blocks.20.attn.proj.weight', 'model.visual.blocks.20.attn.qkv.bias', 'model.visual.blocks.20.attn.qkv.weight', 'model.visual.blocks.20.mlp.fc1.bias', 'model.visual.blocks.20.mlp.fc1.weight', 'model.visual.blocks.20.mlp.fc2.bias', 'model.visual.blocks.20.mlp.fc2.weight', 'model.visual.blocks.20.norm1.bias', 'model.visual.blocks.20.norm1.weight', 'model.visual.blocks.20.norm2.bias', 'model.visual.blocks.20.norm2.weight', 'model.visual.blocks.21.attn.proj.bias', 'model.visual.blocks.21.attn.proj.weight', 'model.visual.blocks.21.attn.qkv.bias', 'model.visual.blocks.21.attn.qkv.weight', 'model.visual.blocks.21.mlp.fc1.bias', 'model.visual.blocks.21.mlp.fc1.weight', 'model.visual.blocks.21.mlp.fc2.bias', 'model.visual.blocks.21.mlp.fc2.weight', 'model.visual.blocks.21.norm1.bias', 'model.visual.blocks.21.norm1.weight', 'model.visual.blocks.21.norm2.bias', 'model.visual.blocks.21.norm2.weight', 'model.visual.blocks.22.attn.proj.bias', 'model.visual.blocks.22.attn.proj.weight', 'model.visual.blocks.22.attn.qkv.bias', 'model.visual.blocks.22.attn.qkv.weight', 'model.visual.blocks.22.mlp.fc1.bias', 'model.visual.blocks.22.mlp.fc1.weight', 'model.visual.blocks.22.mlp.fc2.bias', 'model.visual.blocks.22.mlp.fc2.weight', 'model.visual.blocks.22.norm1.bias', 'model.visual.blocks.22.norm1.weight', 'model.visual.blocks.22.norm2.bias', 'model.visual.blocks.22.norm2.weight', 'model.visual.blocks.23.attn.proj.bias', 'model.visual.blocks.23.attn.proj.weight', 'model.visual.blocks.23.attn.qkv.bias', 'model.visual.blocks.23.attn.qkv.weight', 'model.visual.blocks.23.mlp.fc1.bias', 'model.visual.blocks.23.mlp.fc1.weight', 'model.visual.blocks.23.mlp.fc2.bias', 'model.visual.blocks.23.mlp.fc2.weight', 'model.visual.blocks.23.norm1.bias', 'model.visual.blocks.23.norm1.weight', 'model.visual.blocks.23.norm2.bias', 'model.visual.blocks.23.norm2.weight', 'model.visual.blocks.24.attn.proj.bias', 'model.visual.blocks.24.attn.proj.weight', 'model.visual.blocks.24.attn.qkv.bias', 'model.visual.blocks.24.attn.qkv.weight', 'model.visual.blocks.24.mlp.fc1.bias', 'model.visual.blocks.24.mlp.fc1.weight', 'model.visual.blocks.24.mlp.fc2.bias', 'model.visual.blocks.24.mlp.fc2.weight', 'model.visual.blocks.24.norm1.bias', 'model.visual.blocks.24.norm1.weight', 'model.visual.blocks.24.norm2.bias', 'model.visual.blocks.24.norm2.weight', 'model.visual.blocks.25.attn.proj.bias', 'model.visual.blocks.25.attn.proj.weight', 'model.visual.blocks.25.attn.qkv.bias', 'model.visual.blocks.25.attn.qkv.weight', 'model.visual.blocks.25.mlp.fc1.bias', 'model.visual.blocks.25.mlp.fc1.weight', 'model.visual.blocks.25.mlp.fc2.bias', 'model.visual.blocks.25.mlp.fc2.weight', 'model.visual.blocks.25.norm1.bias', 'model.visual.blocks.25.norm1.weight', 'model.visual.blocks.25.norm2.bias', 'model.visual.blocks.25.norm2.weight', 'model.visual.blocks.26.attn.proj.bias', 'model.visual.blocks.26.attn.proj.weight', 'model.visual.blocks.26.attn.qkv.bias', 'model.visual.blocks.26.attn.qkv.weight', 'model.visual.blocks.26.mlp.fc1.bias', 'model.visual.blocks.26.mlp.fc1.weight', 'model.visual.blocks.26.mlp.fc2.bias', 'model.visual.blocks.26.mlp.fc2.weight', 'model.visual.blocks.26.norm1.bias', 'model.visual.blocks.26.norm1.weight', 'model.visual.blocks.26.norm2.bias', 'model.visual.blocks.26.norm2.weight', 'model.visual.blocks.27.attn.proj.bias', 'model.visual.blocks.27.attn.proj.weight', 'model.visual.blocks.27.attn.qkv.bias', 'model.visual.blocks.27.attn.qkv.weight', 'model.visual.blocks.27.mlp.fc1.bias', 'model.visual.blocks.27.mlp.fc1.weight', 'model.visual.blocks.27.mlp.fc2.bias', 'model.visual.blocks.27.mlp.fc2.weight', 'model.visual.blocks.27.norm1.bias', 'model.visual.blocks.27.norm1.weight', 'model.visual.blocks.27.norm2.bias', 'model.visual.blocks.27.norm2.weight', 'model.visual.blocks.28.attn.proj.bias', 'model.visual.blocks.28.attn.proj.weight', 'model.visual.blocks.28.attn.qkv.bias', 'model.visual.blocks.28.attn.qkv.weight', 'model.visual.blocks.28.mlp.fc1.bias', 'model.visual.blocks.28.mlp.fc1.weight', 'model.visual.blocks.28.mlp.fc2.bias', 'model.visual.blocks.28.mlp.fc2.weight', 'model.visual.blocks.28.norm1.bias', 'model.visual.blocks.28.norm1.weight', 'model.visual.blocks.28.norm2.bias', 'model.visual.blocks.28.norm2.weight', 'model.visual.blocks.29.attn.proj.bias', 'model.visual.blocks.29.attn.proj.weight', 'model.visual.blocks.29.attn.qkv.bias', 'model.visual.blocks.29.attn.qkv.weight', 'model.visual.blocks.29.mlp.fc1.bias', 'model.visual.blocks.29.mlp.fc1.weight', 'model.visual.blocks.29.mlp.fc2.bias', 'model.visual.blocks.29.mlp.fc2.weight', 'model.visual.blocks.29.norm1.bias', 'model.visual.blocks.29.norm1.weight', 'model.visual.blocks.29.norm2.bias', 'model.visual.blocks.29.norm2.weight', 'model.visual.blocks.3.attn.proj.bias', 'model.visual.blocks.3.attn.proj.weight', 'model.visual.blocks.3.attn.qkv.bias', 'model.visual.blocks.3.attn.qkv.weight', 'model.visual.blocks.3.mlp.fc1.bias', 'model.visual.blocks.3.mlp.fc1.weight', 'model.visual.blocks.3.mlp.fc2.bias', 'model.visual.blocks.3.mlp.fc2.weight', 'model.visual.blocks.3.norm1.bias', 'model.visual.blocks.3.norm1.weight', 'model.visual.blocks.3.norm2.bias', 'model.visual.blocks.3.norm2.weight', 'model.visual.blocks.30.attn.proj.bias', 'model.visual.blocks.30.attn.proj.weight', 'model.visual.blocks.30.attn.qkv.bias', 'model.visual.blocks.30.attn.qkv.weight', 'model.visual.blocks.30.mlp.fc1.bias', 'model.visual.blocks.30.mlp.fc1.weight', 'model.visual.blocks.30.mlp.fc2.bias', 'model.visual.blocks.30.mlp.fc2.weight', 'model.visual.blocks.30.norm1.bias', 'model.visual.blocks.30.norm1.weight', 'model.visual.blocks.30.norm2.bias', 'model.visual.blocks.30.norm2.weight', 'model.visual.blocks.31.attn.proj.bias', 'model.visual.blocks.31.attn.proj.weight', 'model.visual.blocks.31.attn.qkv.bias', 'model.visual.blocks.31.attn.qkv.weight', 'model.visual.blocks.31.mlp.fc1.bias', 'model.visual.blocks.31.mlp.fc1.weight', 'model.visual.blocks.31.mlp.fc2.bias', 'model.visual.blocks.31.mlp.fc2.weight', 'model.visual.blocks.31.norm1.bias', 'model.visual.blocks.31.norm1.weight', 'model.visual.blocks.31.norm2.bias', 'model.visual.blocks.31.norm2.weight', 'model.visual.blocks.4.attn.proj.bias', 'model.visual.blocks.4.attn.proj.weight', 'model.visual.blocks.4.attn.qkv.bias', 'model.visual.blocks.4.attn.qkv.weight', 'model.visual.blocks.4.mlp.fc1.bias', 'model.visual.blocks.4.mlp.fc1.weight', 'model.visual.blocks.4.mlp.fc2.bias', 'model.visual.blocks.4.mlp.fc2.weight', 'model.visual.blocks.4.norm1.bias', 'model.visual.blocks.4.norm1.weight', 'model.visual.blocks.4.norm2.bias', 'model.visual.blocks.4.norm2.weight', 'model.visual.blocks.5.attn.proj.bias', 'model.visual.blocks.5.attn.proj.weight', 'model.visual.blocks.5.attn.qkv.bias', 'model.visual.blocks.5.attn.qkv.weight', 'model.visual.blocks.5.mlp.fc1.bias', 'model.visual.blocks.5.mlp.fc1.weight', 'model.visual.blocks.5.mlp.fc2.bias', 'model.visual.blocks.5.mlp.fc2.weight', 'model.visual.blocks.5.norm1.bias', 'model.visual.blocks.5.norm1.weight', 'model.visual.blocks.5.norm2.bias', 'model.visual.blocks.5.norm2.weight', 'model.visual.blocks.6.attn.proj.bias', 'model.visual.blocks.6.attn.proj.weight', 'model.visual.blocks.6.attn.qkv.bias', 'model.visual.blocks.6.attn.qkv.weight', 'model.visual.blocks.6.mlp.fc1.bias', 'model.visual.blocks.6.mlp.fc1.weight', 'model.visual.blocks.6.mlp.fc2.bias', 'model.visual.blocks.6.mlp.fc2.weight', 'model.visual.blocks.6.norm1.bias', 'model.visual.blocks.6.norm1.weight', 'model.visual.blocks.6.norm2.bias', 'model.visual.blocks.6.norm2.weight', 'model.visual.blocks.7.attn.proj.bias', 'model.visual.blocks.7.attn.proj.weight', 'model.visual.blocks.7.attn.qkv.bias', 'model.visual.blocks.7.attn.qkv.weight', 'model.visual.blocks.7.mlp.fc1.bias', 'model.visual.blocks.7.mlp.fc1.weight', 'model.visual.blocks.7.mlp.fc2.bias', 'model.visual.blocks.7.mlp.fc2.weight', 'model.visual.blocks.7.norm1.bias', 'model.visual.blocks.7.norm1.weight', 'model.visual.blocks.7.norm2.bias', 'model.visual.blocks.7.norm2.weight', 'model.visual.blocks.8.attn.proj.bias', 'model.visual.blocks.8.attn.proj.weight', 'model.visual.blocks.8.attn.qkv.bias', 'model.visual.blocks.8.attn.qkv.weight', 'model.visual.blocks.8.mlp.fc1.bias', 'model.visual.blocks.8.mlp.fc1.weight', 'model.visual.blocks.8.mlp.fc2.bias', 'model.visual.blocks.8.mlp.fc2.weight', 'model.visual.blocks.8.norm1.bias', 'model.visual.blocks.8.norm1.weight', 'model.visual.blocks.8.norm2.bias', 'model.visual.blocks.8.norm2.weight', 'model.visual.blocks.9.attn.proj.bias', 'model.visual.blocks.9.attn.proj.weight', 'model.visual.blocks.9.attn.qkv.bias', 'model.visual.blocks.9.attn.qkv.weight', 'model.visual.blocks.9.mlp.fc1.bias', 'model.visual.blocks.9.mlp.fc1.weight', 'model.visual.blocks.9.mlp.fc2.bias', 'model.visual.blocks.9.mlp.fc2.weight', 'model.visual.blocks.9.norm1.bias', 'model.visual.blocks.9.norm1.weight', 'model.visual.blocks.9.norm2.bias', 'model.visual.blocks.9.norm2.weight', 'model.visual.merger.ln_q.bias', 'model.visual.merger.ln_q.weight', 'model.visual.merger.mlp.0.bias', 'model.visual.merger.mlp.0.weight', 'model.visual.merger.mlp.2.bias', 'model.visual.merger.mlp.2.weight', 'model.visual.patch_embed.proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "processor = Qwen2VLProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f5a4c45-8504-4b9c-a87a-8222cc92ea6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading adapter weights from checkpoint-96 led to unexpected keys not found in the model: model.layers.0.mlp.down_proj.lora_A.default.weight, model.layers.0.mlp.down_proj.lora_B.default.weight, model.layers.0.mlp.up_proj.lora_A.default.weight, model.layers.0.mlp.up_proj.lora_B.default.weight, model.layers.0.self_attn.k_proj.lora_A.default.weight, model.layers.0.self_attn.k_proj.lora_B.default.weight, model.layers.0.self_attn.q_proj.lora_A.default.weight, model.layers.0.self_attn.q_proj.lora_B.default.weight, model.layers.0.self_attn.v_proj.lora_A.default.weight, model.layers.0.self_attn.v_proj.lora_B.default.weight, model.layers.1.mlp.down_proj.lora_A.default.weight, model.layers.1.mlp.down_proj.lora_B.default.weight, model.layers.1.mlp.up_proj.lora_A.default.weight, model.layers.1.mlp.up_proj.lora_B.default.weight, model.layers.1.self_attn.k_proj.lora_A.default.weight, model.layers.1.self_attn.k_proj.lora_B.default.weight, model.layers.1.self_attn.q_proj.lora_A.default.weight, model.layers.1.self_attn.q_proj.lora_B.default.weight, model.layers.1.self_attn.v_proj.lora_A.default.weight, model.layers.1.self_attn.v_proj.lora_B.default.weight, model.layers.10.mlp.down_proj.lora_A.default.weight, model.layers.10.mlp.down_proj.lora_B.default.weight, model.layers.10.mlp.up_proj.lora_A.default.weight, model.layers.10.mlp.up_proj.lora_B.default.weight, model.layers.10.self_attn.k_proj.lora_A.default.weight, model.layers.10.self_attn.k_proj.lora_B.default.weight, model.layers.10.self_attn.q_proj.lora_A.default.weight, model.layers.10.self_attn.q_proj.lora_B.default.weight, model.layers.10.self_attn.v_proj.lora_A.default.weight, model.layers.10.self_attn.v_proj.lora_B.default.weight, model.layers.11.mlp.down_proj.lora_A.default.weight, model.layers.11.mlp.down_proj.lora_B.default.weight, model.layers.11.mlp.up_proj.lora_A.default.weight, model.layers.11.mlp.up_proj.lora_B.default.weight, model.layers.11.self_attn.k_proj.lora_A.default.weight, model.layers.11.self_attn.k_proj.lora_B.default.weight, model.layers.11.self_attn.q_proj.lora_A.default.weight, model.layers.11.self_attn.q_proj.lora_B.default.weight, model.layers.11.self_attn.v_proj.lora_A.default.weight, model.layers.11.self_attn.v_proj.lora_B.default.weight, model.layers.12.mlp.down_proj.lora_A.default.weight, model.layers.12.mlp.down_proj.lora_B.default.weight, model.layers.12.mlp.up_proj.lora_A.default.weight, model.layers.12.mlp.up_proj.lora_B.default.weight, model.layers.12.self_attn.k_proj.lora_A.default.weight, model.layers.12.self_attn.k_proj.lora_B.default.weight, model.layers.12.self_attn.q_proj.lora_A.default.weight, model.layers.12.self_attn.q_proj.lora_B.default.weight, model.layers.12.self_attn.v_proj.lora_A.default.weight, model.layers.12.self_attn.v_proj.lora_B.default.weight, model.layers.13.mlp.down_proj.lora_A.default.weight, model.layers.13.mlp.down_proj.lora_B.default.weight, model.layers.13.mlp.up_proj.lora_A.default.weight, model.layers.13.mlp.up_proj.lora_B.default.weight, model.layers.13.self_attn.k_proj.lora_A.default.weight, model.layers.13.self_attn.k_proj.lora_B.default.weight, model.layers.13.self_attn.q_proj.lora_A.default.weight, model.layers.13.self_attn.q_proj.lora_B.default.weight, model.layers.13.self_attn.v_proj.lora_A.default.weight, model.layers.13.self_attn.v_proj.lora_B.default.weight, model.layers.14.mlp.down_proj.lora_A.default.weight, model.layers.14.mlp.down_proj.lora_B.default.weight, model.layers.14.mlp.up_proj.lora_A.default.weight, model.layers.14.mlp.up_proj.lora_B.default.weight, model.layers.14.self_attn.k_proj.lora_A.default.weight, model.layers.14.self_attn.k_proj.lora_B.default.weight, model.layers.14.self_attn.q_proj.lora_A.default.weight, model.layers.14.self_attn.q_proj.lora_B.default.weight, model.layers.14.self_attn.v_proj.lora_A.default.weight, model.layers.14.self_attn.v_proj.lora_B.default.weight, model.layers.15.mlp.down_proj.lora_A.default.weight, model.layers.15.mlp.down_proj.lora_B.default.weight, model.layers.15.mlp.up_proj.lora_A.default.weight, model.layers.15.mlp.up_proj.lora_B.default.weight, model.layers.15.self_attn.k_proj.lora_A.default.weight, model.layers.15.self_attn.k_proj.lora_B.default.weight, model.layers.15.self_attn.q_proj.lora_A.default.weight, model.layers.15.self_attn.q_proj.lora_B.default.weight, model.layers.15.self_attn.v_proj.lora_A.default.weight, model.layers.15.self_attn.v_proj.lora_B.default.weight, model.layers.16.mlp.down_proj.lora_A.default.weight, model.layers.16.mlp.down_proj.lora_B.default.weight, model.layers.16.mlp.up_proj.lora_A.default.weight, model.layers.16.mlp.up_proj.lora_B.default.weight, model.layers.16.self_attn.k_proj.lora_A.default.weight, model.layers.16.self_attn.k_proj.lora_B.default.weight, model.layers.16.self_attn.q_proj.lora_A.default.weight, model.layers.16.self_attn.q_proj.lora_B.default.weight, model.layers.16.self_attn.v_proj.lora_A.default.weight, model.layers.16.self_attn.v_proj.lora_B.default.weight, model.layers.17.mlp.down_proj.lora_A.default.weight, model.layers.17.mlp.down_proj.lora_B.default.weight, model.layers.17.mlp.up_proj.lora_A.default.weight, model.layers.17.mlp.up_proj.lora_B.default.weight, model.layers.17.self_attn.k_proj.lora_A.default.weight, model.layers.17.self_attn.k_proj.lora_B.default.weight, model.layers.17.self_attn.q_proj.lora_A.default.weight, model.layers.17.self_attn.q_proj.lora_B.default.weight, model.layers.17.self_attn.v_proj.lora_A.default.weight, model.layers.17.self_attn.v_proj.lora_B.default.weight, model.layers.18.mlp.down_proj.lora_A.default.weight, model.layers.18.mlp.down_proj.lora_B.default.weight, model.layers.18.mlp.up_proj.lora_A.default.weight, model.layers.18.mlp.up_proj.lora_B.default.weight, model.layers.18.self_attn.k_proj.lora_A.default.weight, model.layers.18.self_attn.k_proj.lora_B.default.weight, model.layers.18.self_attn.q_proj.lora_A.default.weight, model.layers.18.self_attn.q_proj.lora_B.default.weight, model.layers.18.self_attn.v_proj.lora_A.default.weight, model.layers.18.self_attn.v_proj.lora_B.default.weight, model.layers.19.mlp.down_proj.lora_A.default.weight, model.layers.19.mlp.down_proj.lora_B.default.weight, model.layers.19.mlp.up_proj.lora_A.default.weight, model.layers.19.mlp.up_proj.lora_B.default.weight, model.layers.19.self_attn.k_proj.lora_A.default.weight, model.layers.19.self_attn.k_proj.lora_B.default.weight, model.layers.19.self_attn.q_proj.lora_A.default.weight, model.layers.19.self_attn.q_proj.lora_B.default.weight, model.layers.19.self_attn.v_proj.lora_A.default.weight, model.layers.19.self_attn.v_proj.lora_B.default.weight, model.layers.2.mlp.down_proj.lora_A.default.weight, model.layers.2.mlp.down_proj.lora_B.default.weight, model.layers.2.mlp.up_proj.lora_A.default.weight, model.layers.2.mlp.up_proj.lora_B.default.weight, model.layers.2.self_attn.k_proj.lora_A.default.weight, model.layers.2.self_attn.k_proj.lora_B.default.weight, model.layers.2.self_attn.q_proj.lora_A.default.weight, model.layers.2.self_attn.q_proj.lora_B.default.weight, model.layers.2.self_attn.v_proj.lora_A.default.weight, model.layers.2.self_attn.v_proj.lora_B.default.weight, model.layers.20.mlp.down_proj.lora_A.default.weight, model.layers.20.mlp.down_proj.lora_B.default.weight, model.layers.20.mlp.up_proj.lora_A.default.weight, model.layers.20.mlp.up_proj.lora_B.default.weight, model.layers.20.self_attn.k_proj.lora_A.default.weight, model.layers.20.self_attn.k_proj.lora_B.default.weight, model.layers.20.self_attn.q_proj.lora_A.default.weight, model.layers.20.self_attn.q_proj.lora_B.default.weight, model.layers.20.self_attn.v_proj.lora_A.default.weight, model.layers.20.self_attn.v_proj.lora_B.default.weight, model.layers.21.mlp.down_proj.lora_A.default.weight, model.layers.21.mlp.down_proj.lora_B.default.weight, model.layers.21.mlp.up_proj.lora_A.default.weight, model.layers.21.mlp.up_proj.lora_B.default.weight, model.layers.21.self_attn.k_proj.lora_A.default.weight, model.layers.21.self_attn.k_proj.lora_B.default.weight, model.layers.21.self_attn.q_proj.lora_A.default.weight, model.layers.21.self_attn.q_proj.lora_B.default.weight, model.layers.21.self_attn.v_proj.lora_A.default.weight, model.layers.21.self_attn.v_proj.lora_B.default.weight, model.layers.22.mlp.down_proj.lora_A.default.weight, model.layers.22.mlp.down_proj.lora_B.default.weight, model.layers.22.mlp.up_proj.lora_A.default.weight, model.layers.22.mlp.up_proj.lora_B.default.weight, model.layers.22.self_attn.k_proj.lora_A.default.weight, model.layers.22.self_attn.k_proj.lora_B.default.weight, model.layers.22.self_attn.q_proj.lora_A.default.weight, model.layers.22.self_attn.q_proj.lora_B.default.weight, model.layers.22.self_attn.v_proj.lora_A.default.weight, model.layers.22.self_attn.v_proj.lora_B.default.weight, model.layers.23.mlp.down_proj.lora_A.default.weight, model.layers.23.mlp.down_proj.lora_B.default.weight, model.layers.23.mlp.up_proj.lora_A.default.weight, model.layers.23.mlp.up_proj.lora_B.default.weight, model.layers.23.self_attn.k_proj.lora_A.default.weight, model.layers.23.self_attn.k_proj.lora_B.default.weight, model.layers.23.self_attn.q_proj.lora_A.default.weight, model.layers.23.self_attn.q_proj.lora_B.default.weight, model.layers.23.self_attn.v_proj.lora_A.default.weight, model.layers.23.self_attn.v_proj.lora_B.default.weight, model.layers.24.mlp.down_proj.lora_A.default.weight, model.layers.24.mlp.down_proj.lora_B.default.weight, model.layers.24.mlp.up_proj.lora_A.default.weight, model.layers.24.mlp.up_proj.lora_B.default.weight, model.layers.24.self_attn.k_proj.lora_A.default.weight, model.layers.24.self_attn.k_proj.lora_B.default.weight, model.layers.24.self_attn.q_proj.lora_A.default.weight, model.layers.24.self_attn.q_proj.lora_B.default.weight, model.layers.24.self_attn.v_proj.lora_A.default.weight, model.layers.24.self_attn.v_proj.lora_B.default.weight, model.layers.25.mlp.down_proj.lora_A.default.weight, model.layers.25.mlp.down_proj.lora_B.default.weight, model.layers.25.mlp.up_proj.lora_A.default.weight, model.layers.25.mlp.up_proj.lora_B.default.weight, model.layers.25.self_attn.k_proj.lora_A.default.weight, model.layers.25.self_attn.k_proj.lora_B.default.weight, model.layers.25.self_attn.q_proj.lora_A.default.weight, model.layers.25.self_attn.q_proj.lora_B.default.weight, model.layers.25.self_attn.v_proj.lora_A.default.weight, model.layers.25.self_attn.v_proj.lora_B.default.weight, model.layers.26.mlp.down_proj.lora_A.default.weight, model.layers.26.mlp.down_proj.lora_B.default.weight, model.layers.26.mlp.up_proj.lora_A.default.weight, model.layers.26.mlp.up_proj.lora_B.default.weight, model.layers.26.self_attn.k_proj.lora_A.default.weight, model.layers.26.self_attn.k_proj.lora_B.default.weight, model.layers.26.self_attn.q_proj.lora_A.default.weight, model.layers.26.self_attn.q_proj.lora_B.default.weight, model.layers.26.self_attn.v_proj.lora_A.default.weight, model.layers.26.self_attn.v_proj.lora_B.default.weight, model.layers.27.mlp.down_proj.lora_A.default.weight, model.layers.27.mlp.down_proj.lora_B.default.weight, model.layers.27.mlp.up_proj.lora_A.default.weight, model.layers.27.mlp.up_proj.lora_B.default.weight, model.layers.27.self_attn.k_proj.lora_A.default.weight, model.layers.27.self_attn.k_proj.lora_B.default.weight, model.layers.27.self_attn.q_proj.lora_A.default.weight, model.layers.27.self_attn.q_proj.lora_B.default.weight, model.layers.27.self_attn.v_proj.lora_A.default.weight, model.layers.27.self_attn.v_proj.lora_B.default.weight, model.layers.3.mlp.down_proj.lora_A.default.weight, model.layers.3.mlp.down_proj.lora_B.default.weight, model.layers.3.mlp.up_proj.lora_A.default.weight, model.layers.3.mlp.up_proj.lora_B.default.weight, model.layers.3.self_attn.k_proj.lora_A.default.weight, model.layers.3.self_attn.k_proj.lora_B.default.weight, model.layers.3.self_attn.q_proj.lora_A.default.weight, model.layers.3.self_attn.q_proj.lora_B.default.weight, model.layers.3.self_attn.v_proj.lora_A.default.weight, model.layers.3.self_attn.v_proj.lora_B.default.weight, model.layers.4.mlp.down_proj.lora_A.default.weight, model.layers.4.mlp.down_proj.lora_B.default.weight, model.layers.4.mlp.up_proj.lora_A.default.weight, model.layers.4.mlp.up_proj.lora_B.default.weight, model.layers.4.self_attn.k_proj.lora_A.default.weight, model.layers.4.self_attn.k_proj.lora_B.default.weight, model.layers.4.self_attn.q_proj.lora_A.default.weight, model.layers.4.self_attn.q_proj.lora_B.default.weight, model.layers.4.self_attn.v_proj.lora_A.default.weight, model.layers.4.self_attn.v_proj.lora_B.default.weight, model.layers.5.mlp.down_proj.lora_A.default.weight, model.layers.5.mlp.down_proj.lora_B.default.weight, model.layers.5.mlp.up_proj.lora_A.default.weight, model.layers.5.mlp.up_proj.lora_B.default.weight, model.layers.5.self_attn.k_proj.lora_A.default.weight, model.layers.5.self_attn.k_proj.lora_B.default.weight, model.layers.5.self_attn.q_proj.lora_A.default.weight, model.layers.5.self_attn.q_proj.lora_B.default.weight, model.layers.5.self_attn.v_proj.lora_A.default.weight, model.layers.5.self_attn.v_proj.lora_B.default.weight, model.layers.6.mlp.down_proj.lora_A.default.weight, model.layers.6.mlp.down_proj.lora_B.default.weight, model.layers.6.mlp.up_proj.lora_A.default.weight, model.layers.6.mlp.up_proj.lora_B.default.weight, model.layers.6.self_attn.k_proj.lora_A.default.weight, model.layers.6.self_attn.k_proj.lora_B.default.weight, model.layers.6.self_attn.q_proj.lora_A.default.weight, model.layers.6.self_attn.q_proj.lora_B.default.weight, model.layers.6.self_attn.v_proj.lora_A.default.weight, model.layers.6.self_attn.v_proj.lora_B.default.weight, model.layers.7.mlp.down_proj.lora_A.default.weight, model.layers.7.mlp.down_proj.lora_B.default.weight, model.layers.7.mlp.up_proj.lora_A.default.weight, model.layers.7.mlp.up_proj.lora_B.default.weight, model.layers.7.self_attn.k_proj.lora_A.default.weight, model.layers.7.self_attn.k_proj.lora_B.default.weight, model.layers.7.self_attn.q_proj.lora_A.default.weight, model.layers.7.self_attn.q_proj.lora_B.default.weight, model.layers.7.self_attn.v_proj.lora_A.default.weight, model.layers.7.self_attn.v_proj.lora_B.default.weight, model.layers.8.mlp.down_proj.lora_A.default.weight, model.layers.8.mlp.down_proj.lora_B.default.weight, model.layers.8.mlp.up_proj.lora_A.default.weight, model.layers.8.mlp.up_proj.lora_B.default.weight, model.layers.8.self_attn.k_proj.lora_A.default.weight, model.layers.8.self_attn.k_proj.lora_B.default.weight, model.layers.8.self_attn.q_proj.lora_A.default.weight, model.layers.8.self_attn.q_proj.lora_B.default.weight, model.layers.8.self_attn.v_proj.lora_A.default.weight, model.layers.8.self_attn.v_proj.lora_B.default.weight, model.layers.9.mlp.down_proj.lora_A.default.weight, model.layers.9.mlp.down_proj.lora_B.default.weight, model.layers.9.mlp.up_proj.lora_A.default.weight, model.layers.9.mlp.up_proj.lora_B.default.weight, model.layers.9.self_attn.k_proj.lora_A.default.weight, model.layers.9.self_attn.k_proj.lora_B.default.weight, model.layers.9.self_attn.q_proj.lora_A.default.weight, model.layers.9.self_attn.q_proj.lora_B.default.weight, model.layers.9.self_attn.v_proj.lora_A.default.weight, model.layers.9.self_attn.v_proj.lora_B.default.weight. Loading adapter weights from checkpoint-96 led to missing keys in the model: model.language_model.layers.0.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.0.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.0.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.0.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.0.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.0.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.0.mlp.up_proj.lora_A.default.weight, model.language_model.layers.0.mlp.up_proj.lora_B.default.weight, model.language_model.layers.0.mlp.down_proj.lora_A.default.weight, model.language_model.layers.0.mlp.down_proj.lora_B.default.weight, model.language_model.layers.1.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.1.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.1.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.1.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.1.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.1.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.1.mlp.up_proj.lora_A.default.weight, model.language_model.layers.1.mlp.up_proj.lora_B.default.weight, model.language_model.layers.1.mlp.down_proj.lora_A.default.weight, model.language_model.layers.1.mlp.down_proj.lora_B.default.weight, model.language_model.layers.2.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.2.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.2.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.2.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.2.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.2.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.2.mlp.up_proj.lora_A.default.weight, model.language_model.layers.2.mlp.up_proj.lora_B.default.weight, model.language_model.layers.2.mlp.down_proj.lora_A.default.weight, model.language_model.layers.2.mlp.down_proj.lora_B.default.weight, model.language_model.layers.3.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.3.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.3.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.3.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.3.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.3.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.3.mlp.up_proj.lora_A.default.weight, model.language_model.layers.3.mlp.up_proj.lora_B.default.weight, model.language_model.layers.3.mlp.down_proj.lora_A.default.weight, model.language_model.layers.3.mlp.down_proj.lora_B.default.weight, model.language_model.layers.4.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.4.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.4.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.4.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.4.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.4.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.4.mlp.up_proj.lora_A.default.weight, model.language_model.layers.4.mlp.up_proj.lora_B.default.weight, model.language_model.layers.4.mlp.down_proj.lora_A.default.weight, model.language_model.layers.4.mlp.down_proj.lora_B.default.weight, model.language_model.layers.5.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.5.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.5.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.5.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.5.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.5.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.5.mlp.up_proj.lora_A.default.weight, model.language_model.layers.5.mlp.up_proj.lora_B.default.weight, model.language_model.layers.5.mlp.down_proj.lora_A.default.weight, model.language_model.layers.5.mlp.down_proj.lora_B.default.weight, model.language_model.layers.6.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.6.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.6.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.6.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.6.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.6.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.6.mlp.up_proj.lora_A.default.weight, model.language_model.layers.6.mlp.up_proj.lora_B.default.weight, model.language_model.layers.6.mlp.down_proj.lora_A.default.weight, model.language_model.layers.6.mlp.down_proj.lora_B.default.weight, model.language_model.layers.7.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.7.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.7.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.7.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.7.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.7.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.7.mlp.up_proj.lora_A.default.weight, model.language_model.layers.7.mlp.up_proj.lora_B.default.weight, model.language_model.layers.7.mlp.down_proj.lora_A.default.weight, model.language_model.layers.7.mlp.down_proj.lora_B.default.weight, model.language_model.layers.8.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.8.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.8.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.8.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.8.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.8.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.8.mlp.up_proj.lora_A.default.weight, model.language_model.layers.8.mlp.up_proj.lora_B.default.weight, model.language_model.layers.8.mlp.down_proj.lora_A.default.weight, model.language_model.layers.8.mlp.down_proj.lora_B.default.weight, model.language_model.layers.9.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.9.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.9.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.9.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.9.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.9.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.9.mlp.up_proj.lora_A.default.weight, model.language_model.layers.9.mlp.up_proj.lora_B.default.weight, model.language_model.layers.9.mlp.down_proj.lora_A.default.weight, model.language_model.layers.9.mlp.down_proj.lora_B.default.weight, model.language_model.layers.10.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.10.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.10.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.10.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.10.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.10.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.10.mlp.up_proj.lora_A.default.weight, model.language_model.layers.10.mlp.up_proj.lora_B.default.weight, model.language_model.layers.10.mlp.down_proj.lora_A.default.weight, model.language_model.layers.10.mlp.down_proj.lora_B.default.weight, model.language_model.layers.11.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.11.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.11.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.11.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.11.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.11.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.11.mlp.up_proj.lora_A.default.weight, model.language_model.layers.11.mlp.up_proj.lora_B.default.weight, model.language_model.layers.11.mlp.down_proj.lora_A.default.weight, model.language_model.layers.11.mlp.down_proj.lora_B.default.weight, model.language_model.layers.12.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.12.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.12.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.12.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.12.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.12.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.12.mlp.up_proj.lora_A.default.weight, model.language_model.layers.12.mlp.up_proj.lora_B.default.weight, model.language_model.layers.12.mlp.down_proj.lora_A.default.weight, model.language_model.layers.12.mlp.down_proj.lora_B.default.weight, model.language_model.layers.13.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.13.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.13.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.13.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.13.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.13.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.13.mlp.up_proj.lora_A.default.weight, model.language_model.layers.13.mlp.up_proj.lora_B.default.weight, model.language_model.layers.13.mlp.down_proj.lora_A.default.weight, model.language_model.layers.13.mlp.down_proj.lora_B.default.weight, model.language_model.layers.14.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.14.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.14.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.14.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.14.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.14.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.14.mlp.up_proj.lora_A.default.weight, model.language_model.layers.14.mlp.up_proj.lora_B.default.weight, model.language_model.layers.14.mlp.down_proj.lora_A.default.weight, model.language_model.layers.14.mlp.down_proj.lora_B.default.weight, model.language_model.layers.15.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.15.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.15.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.15.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.15.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.15.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.15.mlp.up_proj.lora_A.default.weight, model.language_model.layers.15.mlp.up_proj.lora_B.default.weight, model.language_model.layers.15.mlp.down_proj.lora_A.default.weight, model.language_model.layers.15.mlp.down_proj.lora_B.default.weight, model.language_model.layers.16.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.16.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.16.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.16.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.16.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.16.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.16.mlp.up_proj.lora_A.default.weight, model.language_model.layers.16.mlp.up_proj.lora_B.default.weight, model.language_model.layers.16.mlp.down_proj.lora_A.default.weight, model.language_model.layers.16.mlp.down_proj.lora_B.default.weight, model.language_model.layers.17.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.17.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.17.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.17.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.17.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.17.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.17.mlp.up_proj.lora_A.default.weight, model.language_model.layers.17.mlp.up_proj.lora_B.default.weight, model.language_model.layers.17.mlp.down_proj.lora_A.default.weight, model.language_model.layers.17.mlp.down_proj.lora_B.default.weight, model.language_model.layers.18.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.18.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.18.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.18.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.18.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.18.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.18.mlp.up_proj.lora_A.default.weight, model.language_model.layers.18.mlp.up_proj.lora_B.default.weight, model.language_model.layers.18.mlp.down_proj.lora_A.default.weight, model.language_model.layers.18.mlp.down_proj.lora_B.default.weight, model.language_model.layers.19.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.19.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.19.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.19.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.19.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.19.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.19.mlp.up_proj.lora_A.default.weight, model.language_model.layers.19.mlp.up_proj.lora_B.default.weight, model.language_model.layers.19.mlp.down_proj.lora_A.default.weight, model.language_model.layers.19.mlp.down_proj.lora_B.default.weight, model.language_model.layers.20.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.20.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.20.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.20.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.20.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.20.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.20.mlp.up_proj.lora_A.default.weight, model.language_model.layers.20.mlp.up_proj.lora_B.default.weight, model.language_model.layers.20.mlp.down_proj.lora_A.default.weight, model.language_model.layers.20.mlp.down_proj.lora_B.default.weight, model.language_model.layers.21.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.21.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.21.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.21.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.21.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.21.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.21.mlp.up_proj.lora_A.default.weight, model.language_model.layers.21.mlp.up_proj.lora_B.default.weight, model.language_model.layers.21.mlp.down_proj.lora_A.default.weight, model.language_model.layers.21.mlp.down_proj.lora_B.default.weight, model.language_model.layers.22.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.22.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.22.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.22.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.22.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.22.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.22.mlp.up_proj.lora_A.default.weight, model.language_model.layers.22.mlp.up_proj.lora_B.default.weight, model.language_model.layers.22.mlp.down_proj.lora_A.default.weight, model.language_model.layers.22.mlp.down_proj.lora_B.default.weight, model.language_model.layers.23.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.23.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.23.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.23.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.23.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.23.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.23.mlp.up_proj.lora_A.default.weight, model.language_model.layers.23.mlp.up_proj.lora_B.default.weight, model.language_model.layers.23.mlp.down_proj.lora_A.default.weight, model.language_model.layers.23.mlp.down_proj.lora_B.default.weight, model.language_model.layers.24.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.24.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.24.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.24.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.24.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.24.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.24.mlp.up_proj.lora_A.default.weight, model.language_model.layers.24.mlp.up_proj.lora_B.default.weight, model.language_model.layers.24.mlp.down_proj.lora_A.default.weight, model.language_model.layers.24.mlp.down_proj.lora_B.default.weight, model.language_model.layers.25.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.25.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.25.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.25.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.25.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.25.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.25.mlp.up_proj.lora_A.default.weight, model.language_model.layers.25.mlp.up_proj.lora_B.default.weight, model.language_model.layers.25.mlp.down_proj.lora_A.default.weight, model.language_model.layers.25.mlp.down_proj.lora_B.default.weight, model.language_model.layers.26.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.26.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.26.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.26.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.26.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.26.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.26.mlp.up_proj.lora_A.default.weight, model.language_model.layers.26.mlp.up_proj.lora_B.default.weight, model.language_model.layers.26.mlp.down_proj.lora_A.default.weight, model.language_model.layers.26.mlp.down_proj.lora_B.default.weight, model.language_model.layers.27.self_attn.q_proj.lora_A.default.weight, model.language_model.layers.27.self_attn.q_proj.lora_B.default.weight, model.language_model.layers.27.self_attn.k_proj.lora_A.default.weight, model.language_model.layers.27.self_attn.k_proj.lora_B.default.weight, model.language_model.layers.27.self_attn.v_proj.lora_A.default.weight, model.language_model.layers.27.self_attn.v_proj.lora_B.default.weight, model.language_model.layers.27.mlp.up_proj.lora_A.default.weight, model.language_model.layers.27.mlp.up_proj.lora_B.default.weight, model.language_model.layers.27.mlp.down_proj.lora_A.default.weight, model.language_model.layers.27.mlp.down_proj.lora_B.default.weight\n"
     ]
    }
   ],
   "source": [
    "adapter_path = \"checkpoint-96\"\n",
    "model.load_adapter(adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa1e3ad7-1559-4944-a376-7af90ec6bab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': 'You are a human evaluator choosing between two AI-generated images—image 1 (left) and image 2 (right)—produced from a text prompt. Critically compare both images and choose the better image.'}]},\n",
       " {'role': 'user',\n",
       "  'content': [{'type': 'image',\n",
       "    'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=768x768>},\n",
       "   {'type': 'image',\n",
       "    'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512>},\n",
       "   {'type': 'text',\n",
       "    'text': \"Task Structure:\\n1. In a <think> block, compare the two images on each of five relative criteria:\\nAlignment: Which image matches the prompt better?\\nObject & Scene Correctness: Which has more believable objects and background?\\nImage Quality (Photorealism): Which is sharper and free of defects?\\nAesthetic Appeal: Which has better composition, color, mood?\\nCreativity & Originality: Which is more unique or storytelling?\\n\\nFor each criterion, include a one-sentence comparison and then a Score chosen from:\\nStrongly prefer image 1\\nPrefer image 1\\nBoth images are preferred\\nPrefer image 2\\nStrongly prefer image 2\\n\\n2. In a <preference> block, output your overall choice—either '1' or '2'.. The images are generated from the prompt: hyperrealistic polaroid photograph, poltergeist creature standing over a dead boy in a large bedroom, many appendages, bed, abandoned bedroom, cobwebs, bloodstains on floor, old house, large windows ,\"}]}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92a02839-46ba-4d7e-b05c-f07293472c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'אישור崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高.Abs崇高.Abs崇高.Abs崇高崇高崇高.Abs崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高崇高.Abs崇高崇高@js崇高崇高崇高崇高崇高崇高@js崇高@jsGenerator崇高崇高崇高.Absдерж崇高崇高崇高崇高.Abs TOOL TOOL handful忘崇高@js崇高@js崇高@js崇高@js崇高@js崇高.Abs崇高崇高崇高崇高崇高崇高崇高.Absдерж creditors creditors creditors creditors creditors creditors creditors creditors creditors creditors creditors creditors creditors creditors creditors creditors creditors creditors creditors creditors creditors creditors creditors creditors creditors creditors崇高崇高崇高崇高@js崇高 ******************************************************************************/\\n崇高 ******************************************************************************/\\n崇高崇高Looking avant ******************************************************************************/\\n崇高@js Loy瞀 Loy鹒崇高.Absдерж campus modification崇高崇高崇高崇高崇高Lookingجهادpush崇高Looking avantLooking avantLooking avant ******************************************************************************/\\n崇高Looking말 ******************************************************************************/\\n崇高Looking avant ******************************************************************************/\\n崇高Looking말崇高崇高崇高崇高崇高崇高崇高崇高Looking말崇高Lookingجهاد崇高崇高喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁Ὸ崇高崇高崇高崇高喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁ジーFalse CHARxFFFFFFFFジー喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁喁 LoyGenerator.HealthFalse磺崇高喁.ConnectionStringsaspect pure QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatinDirectoriesเรือน下单妈妈ジー屯崇高뀝喁 Loy eighteen崇高뀝 Loyเรือน下单崇高뀝 Loyเรือน下单 sociétéGeneratorPt QLatin QLatin QLatin QLatin QLatinx崇高뀝ジー QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatin---\\n---\\n/:xFFFFFFFFジー屯뀝มือlope_kernel stadium QLatin lap_sig.setOutput崇高뀝_units-vesm下单_act邳Themeсер哽_CMP.setOutput뀝_units-vesm У lap旋转崇高뀝_units妈妈---\\n.Abs QLatin lap旋转崇高崇高뀝 Loyเรือน专业从事lit TOOLเรือน专业从事 Loy Télé말뀝◤︿말 IID QLatin lap旋转崇高뀝_units妈妈뀝_units妈妈 IIDเรือนGrid campus︿xFFFFFFFF뀝 Loy.setOutput lap喁Ὸジー;};\\n鹒cef崇高({}, Télé︿崇高({},afetyジー屯.Abs蓰 Loy Katrina lap崇高喁_units妈妈ジー.setOutput崇高喁喁喁喁Ὸジーume({}, QLatin lapсер Katrinaernels QLatin IID崇高喁◤ ככהῸ崇高喁◤.espauc Télé앍Generator ships lap旋转崇高喁◤Looking︿るように下单Looking︿崇高喁_units eighteen\\tdeltalope_kernel({}, QLatin lap旋转xFFFFFFFF Katrina喁◤CHAN QLatin laplit QLatin laplit磺崇高喁_units eighteen崇高_null崇高喁崇高喁_units eighteen quaternion商务 mặc pob Télé말.setOutput崇高 У QLatin QLatin trim lap喁(answer_sender\\x04 QLatin QLatin QLatin lapõesジー锭auc TéléxFFFFFFFF POWER뀝_units eighteenpush뀝.setOutput kształジー.setOutputDrivers kicking)v윳 Katrinaernels_act哽紧เรือน bootsQuaternion.setOutput뀝.setOutputlitジー뭘 TéléxFFFFFFFF Burton Katrina喁崇高popoverเรือน pure뀝喁เรือนx QLatin QLatin QLatin QLatin---\\nเรือนx.setOutput Jr QLatin QLatin lap Company Jrジー({}, expires.setOutput lap Companyandard Loy Burton trim.getHeight Télé authorityジーxFFFFFFFF◤ СанxFFFFFFFF◤õesるように({},เรือน yarnהל POWERlope崇高({}, DevComponents.getHeight[List情況말 lap(Is QLatin FDAเรือน QLatin kształ QLatin QLatin QLatin QLatin QLatin QLatin kształ yarnהל({}, conc_logo_act QLatin lap旋转 Уlds gouverเรือน专业从事Drivers_act QLatin trimเรือน专业从事 القطاع TOOL---\\n在玩家中    \\t .setOutput({},ут屯崇高 QLatin kształ QLatin kształ yarnהל({},ут Burton trimเรือนlopeﰮ pob下单 QLatin kształ崇高 manosORIES /^\\\\เรือนrunnerเรือน QLatin kształ yarn({},_actaysiaเรือนrunner.getHeight Télé authority.setOutput꧂ kształ yarnpush hàngῸ Company妈妈 POWERafetyジー TOOL---\\nเรือน QLatin QLatin QLatin laplit崇高({},_act QLatinベン lap eighteenفوض.setOutput Jr QLatin laprunner泡 QLatin lap\\telemlitเรือน专业从事Drivers哽紧Generator利息_CMP妈妈妈妈妈妈 QLatin lap旋转喁เรือน ילדיםут /^\\\\_actlope stadium Jrジー TOOL---\\n بر manos.getHeight喁 QLatin lapсер DevComponents崇高({},_act trimandardジーeselect(Is({},_act崇高ходимxanford QLatin现有的เชี่ยวชา rơi QLatin QLatin QLatin QLatin QLatin QLatin QLatin QLatinxanford鹒SoapSupportﰮ FDAเรือนrunnerregulatedrollback trim꧂_act trimandard◤ expulsionões崇高崇高崇高崇高崇高 WebbxFFFFFFFFﰮﰮﰮﰮﰮ muốnเรือน专业从事Drivers妈妈เชี่ยวชา commoditypush hàngเรือน专业从事Drivers喁เรือน专业从事Drivers_act trimเรือน专业从事Drivers喁(Is Billing.esp GI◤_LOADED Burtonเรือน专业从事 القطاعLooking铷妈妈뀝◤ходим.getHeight鹒 infos QLatinLooking cidadesıyla QLatin Amelia泡妈妈뀝◤    \\t 喁 QLatin kształ IID镳afety /^\\\\_act У Welshﰮﰮﰮﰮﰮ muốn care Amelia-original.setOutputเรือน专业从事Drivers Giov喁 QLatin kształ崇高喁 QLatin kształ崇高말崇高 /^\\\\ QLatin kształ communal QLatin kształ Nintendoafety /^\\\\ Amelia-originalходим sorafety /^\\\\ Amelia /^\\\\ Amelia /^\\\\_act𫭟.Healthinite崇高喁'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = generate_text_from_sample(model, processor, train_dataset[0][:2])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56b68453-2746-47eb-9d34-2b9f39a6e35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fdd3bfa0f3a4ed08987fe70c1303647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "\n",
    "# Paths\n",
    "adapter_path = \"checkpoint-96\"\n",
    "\n",
    "# Load base model\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "processor = Qwen2VLProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Load adapter\n",
    "model = PeftModel.from_pretrained(model, adapter_path)\n",
    "\n",
    "# Merge adapter weights into the base model\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2a2fbcd-78d2-408d-9788-4ab13715e9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\\\n1. Alignment: Image 1 has a more fitting and detailed representation of Rocket Raccoon, with a more realistic and refined appearance. The digital painting style adds to its alignment with the prompt. In contrast, Image 2 has a more stylized and cartoonish representation, which deviates from the prompt.\\\\nScore: Prefer image 1\\\\n\\\\n2. Object and Scene Correctness: Image 1 accurately depicts Rocket Raccoon, with a detailed and realistic representation of his fur, eyes, and overall appearance. Image 2 has a more stylized and cartoonish representation, which deviates from the prompt.\\\\nScore: Prefer image 1\\\\n\\\\n3. Photorealism / Image Quality: Image 1 has a higher level of photorealism and image quality, with a more realistic and refined appearance. Image 2 has a more stylized and cartoonish representation, which deviates from the prompt.\\\\nScore: Prefer image 1\\\\n\\\\n4. Aesthetic Appeal: Image 1 has a strong aesthetic appeal, with a detailed and realistic representation of Rocket Raccoon, good lighting, and a refined digital painting style. Image 2 has a more stylized and cartoonish representation, which deviates from the prompt.\\\\nScore: Prefer image 1\\\\n\\\\n5. Creativity / Originality: Image 1 has a strong creative and originality, with a more realistic and refined representation of Rocket Raccoon, good lighting, and a refined digital painting style. Image 2 has a more stylized and cartoonish representation, which deviates from the prompt.\\\\nScore:Prefer image 1\\\\n</think>\\\\n<preference>0</preference>'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = generate_text_from_sample(model, processor, train_dataset[9])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35a6ff1a-1f24-43fc-b911-6420091927db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = \"./save\"\n",
    "model.save_pretrained(save_path)\n",
    "processor.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5012741a-7e55-41ca-ba56-8a5aa0a9dbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `sft` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `sft`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe3cea61-95dc-408a-9bbe-21918741955d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa1d9fd421e3474b849c3761a817792d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e5f53b11b91482a80fa8171c9886435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/kevinkingslin/sft-qwen2vl-2b/commit/9802f54268d8c2a52f56f7859499993d08ab5a8b', commit_message='Upload processor', commit_description='', oid='9802f54268d8c2a52f56f7859499993d08ab5a8b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/kevinkingslin/sft-qwen2vl-2b', endpoint='https://huggingface.co', repo_type='model', repo_id='kevinkingslin/sft-qwen2vl-2b'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "repo_id = \"kevinkingslin/sft-qwen2vl-2b\"\n",
    "# create_repo(repo_id, private=False)  # Set private=True if needed\n",
    "\n",
    "# model.push_to_hub(repo_id)\n",
    "processor.push_to_hub(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df5e0181-26fd-41f9-9512-c61f058939dc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    },\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true,\n",
      "        \"loss_scale\": 0,\n",
      "        \"loss_scale_window\": 1000,\n",
      "        \"initial_scale_power\": 16,\n",
      "        \"hysteresis\": 2\n",
      "    },\n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\",\n",
      "        \"params\": {\n",
      "            \"lr\": \"auto\",\n",
      "            \"weight_decay\": \"auto\",\n",
      "            \"torch_adam\": true,\n",
      "            \"adam_w_mode\": true\n",
      "        }\n",
      "    },\n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2,\n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\",\n",
      "            \"pin_memory\": true\n",
      "        },\n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\",\n",
      "            \"pin_memory\": true\n",
      "        },\n",
      "        \"overlap_comm\": false,\n",
      "        \"contiguous_gradients\": true,\n",
      "        \"sub_group_size\": 1e6,\n",
      "        \"reduce_bucket_size\": 5e7,\n",
      "        \"allgather_partitions\": true,\n",
      "        \"allgather_bucket_size\": 1e8\n",
      "    },\n",
      "    \"flops_profiler\": {\n",
      "        \"enabled\": false\n",
      "    },\n",
      "\n",
      "    \"gradient_accumulation_steps\": \"auto\",\n",
      "    \"gradient_clipping\": \"auto\",\n",
      "    \"steps_per_print\": 100,\n",
      "    \"train_batch_size\": \"auto\",\n",
      "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
      "    \"wall_clock_breakdown\": false\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat zero3.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ee92139-0538-451d-807d-f7fe4d75e018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting zero3.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile zero3.json\n",
    "{\n",
    "    \"fp16\": {\n",
    "        \"enabled\": false\n",
    "    },\n",
    "    \"bf16\": {\n",
    "        \"enabled\": true,\n",
    "        \"loss_scale\": 0,\n",
    "        \"loss_scale_window\": 1000,\n",
    "        \"initial_scale_power\": 16,\n",
    "        \"hysteresis\": 2\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": \"auto\",\n",
    "            \"weight_decay\": \"auto\",\n",
    "            \"torch_adam\": true,\n",
    "            \"adam_w_mode\": true\n",
    "        }\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"offload_optimizer\": {\n",
    "            \"device\": \"none\",\n",
    "            \"pin_memory\": true\n",
    "        },\n",
    "        \"offload_param\": {\n",
    "            \"device\": \"none\",\n",
    "            \"pin_memory\": true\n",
    "        },\n",
    "        \"overlap_comm\": false,\n",
    "        \"contiguous_gradients\": true,\n",
    "        \"sub_group_size\": 1e6,\n",
    "        \"reduce_bucket_size\": 2e7,\n",
    "        \"allgather_partitions\": true,\n",
    "        \"allgather_bucket_size\": 2e7\n",
    "    },\n",
    "    \"flops_profiler\": {\n",
    "        \"enabled\": false\n",
    "    },\n",
    "\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"gradient_clipping\": \"auto\",\n",
    "    \"steps_per_print\": 100,\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"wall_clock_breakdown\": false\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f60644c-3cf3-429c-8c58-66d124380b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
      "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
      "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
      "\u001b[33mhint: \u001b[m\n",
      "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
      "\u001b[33mhint: \u001b[m\n",
      "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
      "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
      "\u001b[33mhint: \u001b[m\n",
      "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
      "Initialized empty Git repository in /workspace/.git/\n"
     ]
    }
   ],
   "source": [
    "!git init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd205f78-9b35-4704-b85e-fd77b3da008b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main (root-commit) 6157600] first commit\n",
      " 1 file changed, 0 insertions(+), 0 deletions(-)\n",
      " create mode 100644 README.md\n",
      "error: remote origin already exists.\n",
      "Username for 'https://github.com': ^C\n"
     ]
    }
   ],
   "source": [
    "!git add README.md\n",
    "!git commit -m \"first commit\"\n",
    "!git branch -M main\n",
    "!git remote add origin https://github.com/KevinKingslin/qwenfinetune.git\n",
    "!git push -u origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6b78f75-2cbc-488b-a874-e11e44848a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.email \"kevinkingslin@gmail.com\"\n",
    "!git config --global user.name \"Kevin Kingslin\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
