# Model args
model_name_or_path: "Qwen/Qwen2-VL-2B-Instruct"
# model_name_or_path: "Qwen/Qwen2.5-VL-3B-Instruct"
# model_name_or_path: "HuggingFaceTB/SmolVLM-Base"
torch_dtype: bfloat16


# Dataset args
dataset_name: "kevinkingslin/GRPODataset2"

# Trainer args
bf16: true
gradient_checkpointing: false
output_dir: "output/qwen_v6"
per_device_train_batch_size: 4
gradient_accumulation_steps: 1
# max_prompt_length: 1024
max_completion_length: 1024
logging_steps: 1
report_to: "wandb"
lr_scheduler_type: "cosine"
attn_implementation: "flash_attention_2"
max_pixels: 201000
num_train_epochs: 2
run_name: "Qwen-2B_v6"
learning_rate: 2e-5
warmup_steps: 75
save_steps: 25
beta: 0.01
save_only_model: false
num_generations: 2
dataloader_num_workers: 8
dataloader_prefetch_factor: 4
dataloader_pin_memory: true
temperature: 1.0